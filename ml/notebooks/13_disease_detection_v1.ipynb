{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut Disease Detection Model v1\n",
    "\n",
    "## 4-Class Classification: Leaf Rot, Leaf Spot, Healthy, Not Coconut\n",
    "\n",
    "**Author:** Coconut Health Monitor Team  \n",
    "**Date:** January 2026  \n",
    "**Model:** EfficientNetB0 (Transfer Learning)  \n",
    "\n",
    "### Objectives:\n",
    "1. Train a disease detection model for coconut leaves\n",
    "2. Prevent data leaking and overfitting\n",
    "3. Achieve balanced Precision, Recall, F1-score across all classes\n",
    "4. Ensure accuracy is close to F1-score\n",
    "\n",
    "### Classes:\n",
    "- `healthy` - Healthy coconut leaves\n",
    "- `Leaf Rot` - Leaves affected by rot disease\n",
    "- `Leaf_Spot` - Leaves with spot disease\n",
    "- `not_cocount` - Non-coconut images (rejection class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Check GPU\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_PATH = r'D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\data\\raw\\stage_2_split'\n",
    "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
    "VAL_PATH = os.path.join(BASE_PATH, 'val')\n",
    "TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
    "\n",
    "# Model save path\n",
    "MODEL_SAVE_PATH = r'D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\models\\disease_detection_v1'\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50  # Will use early stopping\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "# Class names (alphabetical order - how ImageDataGenerator loads them)\n",
    "CLASS_NAMES = ['Leaf Rot', 'Leaf_Spot', 'healthy', 'not_cocount']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(path):\n",
    "    \"\"\"Count images in each class folder\"\"\"\n",
    "    counts = {}\n",
    "    for class_name in os.listdir(path):\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            counts[class_name] = len([f for f in os.listdir(class_path) \n",
    "                                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    return counts\n",
    "\n",
    "# Count images in each split\n",
    "train_counts = count_images(TRAIN_PATH)\n",
    "val_counts = count_images(VAL_PATH)\n",
    "test_counts = count_images(TEST_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_counts = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Validation': val_counts,\n",
    "    'Test': test_counts\n",
    "}).T\n",
    "\n",
    "df_counts['Total'] = df_counts.sum(axis=1)\n",
    "print(df_counts)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Total Train Images: {sum(train_counts.values())}\")\n",
    "print(f\"Total Validation Images: {sum(val_counts.values())}\")\n",
    "print(f\"Total Test Images: {sum(test_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Train distribution\n",
    "axes[0].bar(train_counts.keys(), train_counts.values(), color=['#2ecc71', '#e74c3c', '#3498db', '#9b59b6'])\n",
    "axes[0].set_title('Training Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Number of Images')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, (k, v) in enumerate(train_counts.items()):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Validation distribution\n",
    "axes[1].bar(val_counts.keys(), val_counts.values(), color=['#2ecc71', '#e74c3c', '#3498db', '#9b59b6'])\n",
    "axes[1].set_title('Validation Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Number of Images')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "for i, (k, v) in enumerate(val_counts.items()):\n",
    "    axes[1].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Test distribution\n",
    "axes[2].bar(test_counts.keys(), test_counts.values(), color=['#2ecc71', '#e74c3c', '#3498db', '#9b59b6'])\n",
    "axes[2].set_title('Test Set Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Number of Images')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "for i, (k, v) in enumerate(test_counts.items()):\n",
    "    axes[2].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_SAVE_PATH, 'dataset_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Loading with Augmentation\n",
    "\n",
    "**Important:** \n",
    "- Training data gets augmentation (rotation, flip, zoom, etc.)\n",
    "- Validation and Test data only get rescaling (NO augmentation to prevent data leaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator WITH augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation and Test data generator - ONLY rescaling (NO augmentation!)\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "print(\"Data generators created successfully!\")\n",
    "print(\"- Training: With augmentation (rotation, flip, zoom, shift)\")\n",
    "print(\"- Validation/Test: Only rescaling (no augmentation to prevent data leaking)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Load validation data (NO augmentation)\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Don't shuffle for consistent evaluation\n",
    ")\n",
    "\n",
    "# Load test data (NO augmentation)\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Don't shuffle for consistent evaluation\n",
    ")\n",
    "\n",
    "# Get class indices\n",
    "print(\"\\nClass Indices (alphabetical order):\")\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "# Store class names in correct order\n",
    "CLASS_NAMES = list(train_generator.class_indices.keys())\n",
    "print(f\"\\nClass Names: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training images with augmentation\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get a batch of images\n",
    "sample_batch, sample_labels = next(train_generator)\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(sample_batch[i])\n",
    "    class_idx = np.argmax(sample_labels[i])\n",
    "    axes[i].set_title(f'Class: {CLASS_NAMES[class_idx]}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Training Images (with Augmentation)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_SAVE_PATH, 'sample_images.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Calculate Class Weights\n",
    "\n",
    "Handle class imbalance by computing class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "train_labels = train_generator.classes\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class Weights (to handle imbalance):\")\n",
    "print(\"=\" * 40)\n",
    "for idx, weight in class_weight_dict.items():\n",
    "    print(f\"  {CLASS_NAMES[idx]}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\n(Higher weight = less samples = model pays more attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Build Model Architecture\n",
    "\n",
    "Using **EfficientNetB0** with Transfer Learning:\n",
    "- Pre-trained on ImageNet\n",
    "- Fine-tune top layers\n",
    "- Add dropout for regularization\n",
    "- Add L2 regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, img_size=224, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Build EfficientNetB0 model with custom classification head\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        img_size: Input image size\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNetB0\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(img_size, img_size, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build custom classification head\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Global Average Pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers with regularization\n",
    "    x = layers.Dense(\n",
    "        256, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.01)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(\n",
    "        128, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.01)\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build the model\n",
    "model, base_model = build_model(\n",
    "    num_classes=NUM_CLASSES, \n",
    "    img_size=IMG_SIZE, \n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "print(\"Model Architecture Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"  Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"  Loss: Categorical Crossentropy\")\n",
    "print(f\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Setup Callbacks\n",
    "\n",
    "Callbacks to prevent overfitting and save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Early stopping - stop if validation loss doesn't improve\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model based on validation accuracy\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(MODEL_SAVE_PATH, 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when validation loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  1. EarlyStopping (patience=10, monitor=val_loss)\")\n",
    "print(\"  2. ModelCheckpoint (save best model by val_accuracy)\")\n",
    "print(\"  3. ReduceLROnPlateau (factor=0.5, patience=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Phase 1: Train with Frozen Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: Training with Frozen Base Model\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Training only the classification head...\")\n",
    "print(f\"Base model layers: {len(base_model.layers)} (all frozen)\")\n",
    "print()\n",
    "\n",
    "# Train Phase 1\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,  # Initial training\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 1 training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Phase 2: Fine-tune Top Layers of Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: Fine-tuning Top Layers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unfreeze the top layers of the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last 20\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Count trainable layers\n",
    "trainable_layers = sum(1 for layer in base_model.layers if layer.trainable)\n",
    "print(f\"Unfrozen top layers: {trainable_layers}\")\n",
    "\n",
    "# Re-compile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE / 10),  # Lower LR for fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"New learning rate: {LEARNING_RATE / 10}\")\n",
    "print(\"\\nStarting fine-tuning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase 2 (Fine-tuning)\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    initial_epoch=len(history_phase1.history['loss']),\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 2 fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "def combine_histories(h1, h2):\n",
    "    \"\"\"Combine two training histories\"\"\"\n",
    "    combined = {}\n",
    "    for key in h1.history.keys():\n",
    "        combined[key] = h1.history[key] + h2.history[key]\n",
    "    return combined\n",
    "\n",
    "history = combine_histories(history_phase1, history_phase2)\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].axvline(x=len(history_phase1.history['loss'])-1, color='r', linestyle='--', label='Fine-tuning Start')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].axvline(x=len(history_phase1.history['loss'])-1, color='r', linestyle='--', label='Fine-tuning Start')\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_SAVE_PATH, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print best metrics\n",
    "best_val_acc = max(history['val_accuracy'])\n",
    "best_epoch = history['val_accuracy'].index(best_val_acc) + 1\n",
    "print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f} (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Load Best Model and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join(MODEL_SAVE_PATH, 'best_model.keras')\n",
    "model = keras.models.load_model(best_model_path)\n",
    "print(f\"Loaded best model from: {best_model_path}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Get predicted classes\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(f\"Total test samples: {len(y_true)}\")\n",
    "print(f\"Total predictions: {len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Class-wise Metrics (Precision, Recall, F1-Score)\n",
    "\n",
    "**Key Requirement:** Precision, Recall, and F1-score should be close to each other for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFICATION REPORT (Class-wise Metrics)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=CLASS_NAMES,\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Get metrics as dictionary for analysis\n",
    "report_dict = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    target_names=CLASS_NAMES, \n",
    "    output_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for better visualization\n",
    "metrics_data = []\n",
    "for class_name in CLASS_NAMES:\n",
    "    metrics_data.append({\n",
    "        'Class': class_name,\n",
    "        'Precision': report_dict[class_name]['precision'],\n",
    "        'Recall': report_dict[class_name]['recall'],\n",
    "        'F1-Score': report_dict[class_name]['f1-score'],\n",
    "        'Support': report_dict[class_name]['support']\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Calculate metric differences\n",
    "metrics_df['P-R Diff'] = abs(metrics_df['Precision'] - metrics_df['Recall'])\n",
    "metrics_df['P-F1 Diff'] = abs(metrics_df['Precision'] - metrics_df['F1-Score'])\n",
    "metrics_df['R-F1 Diff'] = abs(metrics_df['Recall'] - metrics_df['F1-Score'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASS-WISE METRICS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Check if metrics are balanced\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRICS BALANCE CHECK\")\n",
    "print(\"=\" * 70)\n",
    "max_diff = max(metrics_df['P-R Diff'].max(), metrics_df['P-F1 Diff'].max(), metrics_df['R-F1 Diff'].max())\n",
    "print(f\"Maximum Precision-Recall Difference: {metrics_df['P-R Diff'].max():.4f}\")\n",
    "print(f\"Maximum Precision-F1 Difference: {metrics_df['P-F1 Diff'].max():.4f}\")\n",
    "print(f\"Maximum Recall-F1 Difference: {metrics_df['R-F1 Diff'].max():.4f}\")\n",
    "\n",
    "if max_diff < 0.10:\n",
    "    print(\"\\n✅ GOOD: Metrics are well balanced (difference < 10%)\")\n",
    "elif max_diff < 0.15:\n",
    "    print(\"\\n⚠️ ACCEPTABLE: Metrics are reasonably balanced (difference < 15%)\")\n",
    "else:\n",
    "    print(\"\\n❌ WARNING: Metrics have significant imbalance (difference > 15%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class-wise metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, metrics_df['Precision'], width, label='Precision', color='#3498db')\n",
    "bars2 = ax.bar(x, metrics_df['Recall'], width, label='Recall', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, metrics_df['F1-Score'], width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Class-wise Precision, Recall, and F1-Score', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=15)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_SAVE_PATH, 'class_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix (Normalized %)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_SAVE_PATH, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Overall Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "overall_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "overall_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "overall_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Test Accuracy:       {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Weighted Precision:  {overall_precision:.4f} ({overall_precision*100:.2f}%)\")\n",
    "print(f\"  Weighted Recall:     {overall_recall:.4f} ({overall_recall*100:.2f}%)\")\n",
    "print(f\"  Weighted F1-Score:   {overall_f1:.4f} ({overall_f1*100:.2f}%)\")\n",
    "print(f\"  Macro F1-Score:      {macro_f1:.4f} ({macro_f1*100:.2f}%)\")\n",
    "\n",
    "# Check accuracy vs F1 difference\n",
    "acc_f1_diff = abs(test_accuracy - overall_f1)\n",
    "print(f\"\\n  Accuracy - F1 Diff:  {acc_f1_diff:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REQUIREMENTS CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check requirements\n",
    "if acc_f1_diff < 0.05:\n",
    "    print(\"✅ Accuracy is close to F1-Score (diff < 5%)\")\n",
    "else:\n",
    "    print(f\"⚠️ Accuracy and F1-Score have some difference ({acc_f1_diff:.2%})\")\n",
    "\n",
    "# Check class balance\n",
    "f1_scores = [report_dict[c]['f1-score'] for c in CLASS_NAMES]\n",
    "f1_std = np.std(f1_scores)\n",
    "if f1_std < 0.10:\n",
    "    print(\"✅ F1-Scores are similar across classes (std < 10%)\")\n",
    "else:\n",
    "    print(f\"⚠️ F1-Scores vary across classes (std = {f1_std:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overall metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score (W)', 'F1-Score (M)']\n",
    "values = [test_accuracy, overall_precision, overall_recall, overall_f1, macro_f1]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12']\n",
    "\n",
    "bars = ax.bar(metrics, values, color=colors)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.annotate(f'{val:.2%}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, val),\n",
    "                xytext=(0, 5),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Overall Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_SAVE_PATH, 'overall_metrics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Save Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model information\n",
    "model_info = {\n",
    "    'model_name': 'Coconut Disease Detection Model',\n",
    "    'version': 'v1',\n",
    "    'architecture': 'EfficientNetB0 (Transfer Learning)',\n",
    "    'input_size': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'classes': CLASS_NAMES,\n",
    "    'class_indices': train_generator.class_indices,\n",
    "    'performance': {\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'weighted_precision': float(overall_precision),\n",
    "        'weighted_recall': float(overall_recall),\n",
    "        'weighted_f1': float(overall_f1),\n",
    "        'macro_f1': float(macro_f1)\n",
    "    },\n",
    "    'class_metrics': {\n",
    "        class_name: {\n",
    "            'precision': float(report_dict[class_name]['precision']),\n",
    "            'recall': float(report_dict[class_name]['recall']),\n",
    "            'f1_score': float(report_dict[class_name]['f1-score']),\n",
    "            'support': int(report_dict[class_name]['support'])\n",
    "        } for class_name in CLASS_NAMES\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'initial_learning_rate': LEARNING_RATE,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'augmentation': True,\n",
    "        'class_weights': {CLASS_NAMES[k]: float(v) for k, v in class_weight_dict.items()}\n",
    "    },\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_epochs': len(history['loss']),\n",
    "    'best_epoch': best_epoch\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "info_path = os.path.join(MODEL_SAVE_PATH, 'model_info.json')\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"Model info saved to: {info_path}\")\n",
    "print(\"\\nModel Info Summary:\")\n",
    "print(json.dumps(model_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                    TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Model: Coconut Disease Detection v1\n",
    "Architecture: EfficientNetB0 (Transfer Learning)\n",
    "\n",
    "Classes ({NUM_CLASSES}):\n",
    "  - Leaf Rot (Coconut leaf rot disease)\n",
    "  - Leaf_Spot (Coconut leaf spot disease)\n",
    "  - healthy (Healthy coconut leaves)\n",
    "  - not_cocount (Non-coconut images - rejection class)\n",
    "\n",
    "Performance:\n",
    "  ┌──────────────────┬──────────────┐\n",
    "  │ Metric           │ Value        │\n",
    "  ├──────────────────┼──────────────┤\n",
    "  │ Test Accuracy    │ {test_accuracy:.2%}      │\n",
    "  │ Precision (W)    │ {overall_precision:.2%}      │\n",
    "  │ Recall (W)       │ {overall_recall:.2%}      │\n",
    "  │ F1-Score (W)     │ {overall_f1:.2%}      │\n",
    "  │ F1-Score (Macro) │ {macro_f1:.2%}      │\n",
    "  └──────────────────┴──────────────┘\n",
    "\n",
    "Files Saved:\n",
    "  - {os.path.join(MODEL_SAVE_PATH, 'best_model.keras')}\n",
    "  - {os.path.join(MODEL_SAVE_PATH, 'model_info.json')}\n",
    "  - {os.path.join(MODEL_SAVE_PATH, 'training_history.png')}\n",
    "  - {os.path.join(MODEL_SAVE_PATH, 'confusion_matrix.png')}\n",
    "  - {os.path.join(MODEL_SAVE_PATH, 'class_metrics.png')}\n",
    "  - {os.path.join(MODEL_SAVE_PATH, 'overall_metrics.png')}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"                          ✅ TRAINING SUCCESSFUL\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
