{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut Mite Detection - 3-Class Model v9 (Robust & Honest)\n",
    "\n",
    "## Key Improvements from v8:\n",
    "- **Clean Dataset**: v4_clean with no data leaks or corrupted files\n",
    "- **Class Weights**: Computed from training data to handle imbalance\n",
    "- **Focal Loss**: Better handling of hard examples\n",
    "- **Honest Evaluation**: No tricks, real performance metrics\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model | EfficientNetB0 (Transfer Learning) |\n",
    "| Classes | coconut_mite, healthy, not_coconut |\n",
    "| Dataset | v4_clean (13,781 images, no leaks) |\n",
    "| Loss | Focal Loss with class weights |\n",
    "\n",
    "---\n",
    "**Author:** Research Team  \n",
    "**Date:** 2025-12-25  \n",
    "**Version:** v9 (Robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ENVIRONMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  TensorFlow: {tf.__version__}\")\n",
    "print(f\"  GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. CONFIGURATION\n",
    "# ============================================================\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'raw', 'pest_mite', 'dataset_v4_clean')  # CLEAN DATASET!\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models', 'coconut_mite_v9')\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'validation')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Hyperparameters - Conservative for honest evaluation\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # More epochs, rely on early stopping\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_REG = 0.01\n",
    "PATIENCE = 10  # More patience for better convergence\n",
    "\n",
    "CLASS_NAMES = ['coconut_mite', 'healthy', 'not_coconut']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  CONFIGURATION - V9 ROBUST\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Dataset: {DATA_DIR}\")\n",
    "print(f\"  Model:   {MODEL_DIR}\")\n",
    "print(f\"\\n  Hyperparameters:\")\n",
    "print(f\"    Image Size:    {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"    Batch Size:    {BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs:    {EPOCHS}\")\n",
    "print(f\"    Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"    Dropout:       {DROPOUT_RATE}\")\n",
    "print(f\"    L2 Reg:        {L2_REG}\")\n",
    "print(f\"    Early Stop:    {PATIENCE} epochs\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. DATASET VERIFICATION\n",
    "# ============================================================\n",
    "def count_images(directory):\n",
    "    counts = {}\n",
    "    for cls in CLASS_NAMES:\n",
    "        cls_dir = os.path.join(directory, cls)\n",
    "        if os.path.exists(cls_dir):\n",
    "            counts[cls] = len([f for f in os.listdir(cls_dir) \n",
    "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        else:\n",
    "            counts[cls] = 0\n",
    "    return counts\n",
    "\n",
    "train_counts = count_images(TRAIN_DIR)\n",
    "val_counts = count_images(VAL_DIR)\n",
    "test_counts = count_images(TEST_DIR)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  DATASET v4_clean (NO DATA LEAKS!)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  {'Split':<12} {'mite':>8} {'healthy':>10} {'not_coco':>12} {'Total':>8}\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "\n",
    "for split, counts in [('train', train_counts), ('validation', val_counts), ('test', test_counts)]:\n",
    "    total = sum(counts.values())\n",
    "    print(f\"  {split:<12} {counts['coconut_mite']:>8} {counts['healthy']:>10} {counts['not_coconut']:>12} {total:>8}\")\n",
    "\n",
    "total_all = sum(train_counts.values()) + sum(val_counts.values()) + sum(test_counts.values())\n",
    "print(\"  \" + \"-\" * 50)\n",
    "print(f\"  {'TOTAL':<12} {train_counts['coconut_mite']+val_counts['coconut_mite']+test_counts['coconut_mite']:>8} \"\n",
    "      f\"{train_counts['healthy']+val_counts['healthy']+test_counts['healthy']:>10} \"\n",
    "      f\"{train_counts['not_coconut']+val_counts['not_coconut']+test_counts['not_coconut']:>12} {total_all:>8}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. COMPUTE CLASS WEIGHTS\n",
    "# ============================================================\n",
    "# This helps the model pay equal attention to all classes\n",
    "\n",
    "train_labels = []\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    train_labels.extend([i] * train_counts[cls])\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weights = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  CLASS WEIGHTS (for balanced learning)\")\n",
    "print(\"=\" * 60)\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {cls}: {class_weights[i]:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. DATA GENERATORS\n",
    "# ============================================================\n",
    "\n",
    "# Moderate augmentation - not too aggressive\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  DATA GENERATORS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Train: {train_generator.samples} samples, {len(train_generator)} batches\")\n",
    "print(f\"  Val:   {val_generator.samples} samples\")\n",
    "print(f\"  Test:  {test_generator.samples} samples\")\n",
    "print(f\"\\n  Class indices: {train_generator.class_indices}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. FOCAL LOSS (Better for imbalanced/hard examples)\n",
    "# ============================================================\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss for handling class imbalance and hard examples.\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Cross entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Focal weight\n",
    "        weight = tf.pow(1 - y_pred, self.gamma) * y_true\n",
    "        \n",
    "        # Apply alpha if provided\n",
    "        if self.alpha is not None:\n",
    "            weight = weight * self.alpha\n",
    "        \n",
    "        focal_loss = weight * ce\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "print(\"Focal Loss defined with gamma=2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. BUILD MODEL\n",
    "# ============================================================\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build EfficientNetB0 for 3-class classification.\"\"\"\n",
    "    \n",
    "    # Base model\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Custom head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    # Compile with focal loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=FocalLoss(gamma=2.0),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = build_model()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Non-trainable: {model.count_params() - trainable:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8. CALLBACKS\n",
    "# ============================================================\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "print(\"Callbacks configured: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9. PHASE 1: TRAIN WITH FROZEN BASE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  PHASE 1: Training with frozen EfficientNetB0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Initial training\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nPhase 1 completed in {phase1_time:.1f} minutes\")\n",
    "print(f\"Best val_accuracy: {max(history1.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. PHASE 2: FINE-TUNE TOP LAYERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  PHASE 2: Fine-tuning top layers of EfficientNetB0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze top 20 layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE / 10),\n",
    "    loss=FocalLoss(gamma=2.0),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"Trainable params after unfreezing: {trainable:,}\")\n",
    "\n",
    "# Continue training\n",
    "start_time = datetime.now()\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase2_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nPhase 2 completed in {phase2_time:.1f} minutes\")\n",
    "print(f\"Best val_accuracy: {max(history2.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. COMBINE TRAINING HISTORY\n",
    "# ============================================================\n",
    "\n",
    "history = {\n",
    "    'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n",
    "    'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],\n",
    "    'loss': history1.history['loss'] + history2.history['loss'],\n",
    "    'val_loss': history1.history['val_loss'] + history2.history['val_loss']\n",
    "}\n",
    "\n",
    "total_training_time = phase1_time + phase2_time\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(MODEL_DIR, 'training_history.json'), 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f, indent=2)\n",
    "\n",
    "print(f\"Total training time: {total_training_time:.1f} minutes\")\n",
    "print(f\"Total epochs: {len(history['accuracy'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. PLOT TRAINING HISTORY\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, [x*100 for x in history['accuracy']], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, [x*100 for x in history['val_accuracy']], 'r-', label='Validation', linewidth=2)\n",
    "axes[0].axvline(x=len(history1.history['accuracy']), color='green', linestyle='--', alpha=0.5, label='Fine-tune start')\n",
    "\n",
    "best_epoch = np.argmax(history['val_accuracy']) + 1\n",
    "best_val_acc = max(history['val_accuracy']) * 100\n",
    "axes[0].scatter([best_epoch], [best_val_acc], color='green', s=100, zorder=5)\n",
    "axes[0].annotate(f'Best: {best_val_acc:.1f}%', xy=(best_epoch, best_val_acc), \n",
    "                 xytext=(best_epoch+2, best_val_acc-3), fontsize=10)\n",
    "\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "axes[1].axvline(x=len(history1.history['loss']), color='green', linestyle='--', alpha=0.5, label='Fine-tune start')\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate gap\n",
    "final_train = history['accuracy'][-1] * 100\n",
    "final_val = history['val_accuracy'][-1] * 100\n",
    "gap = abs(final_train - final_val)\n",
    "print(f\"\\nFinal Train: {final_train:.2f}%, Val: {final_val:.2f}%, Gap: {gap:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13. LOAD BEST MODEL & EVALUATE ON TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  TEST SET EVALUATION (HONEST METRICS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(\n",
    "    os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    custom_objects={'FocalLoss': FocalLoss}\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "test_generator.reset()\n",
    "y_probs = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Class names in correct order\n",
    "class_names_ordered = list(test_generator.class_indices.keys())\n",
    "\n",
    "print(f\"\\nTest samples: {len(y_true)}\")\n",
    "print(f\"Classes: {class_names_ordered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 14. CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_ordered, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 15. DETAILED METRICS\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  DETAILED METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n  Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"  Macro F1-Score:   {macro_f1*100:.2f}%\")\n",
    "\n",
    "print(\"\\n  Per-Class:\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    print(f\"  {cls}:\")\n",
    "    print(f\"    Precision: {precision[i]*100:.2f}%\")\n",
    "    print(f\"    Recall:    {recall[i]*100:.2f}%\")\n",
    "    print(f\"    F1-Score:  {f1[i]*100:.2f}%\")\n",
    "    print(f\"    Support:   {support[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 16. CONFUSION MATRIX\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Normalized\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 17. REQUIREMENTS CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  UTHPALA MISS REQUIREMENTS CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_pass = True\n",
    "\n",
    "# Req 1: P/R/F1 balanced per class\n",
    "print(\"\\n  [1] P/R/F1 Balanced per Class (gap < 0.10)\")\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    gap = max(precision[i], recall[i], f1[i]) - min(precision[i], recall[i], f1[i])\n",
    "    status = \"PASS\" if gap < 0.10 else \"FAIL\"\n",
    "    if gap >= 0.10:\n",
    "        all_pass = False\n",
    "    print(f\"      {cls}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f} | Gap={gap:.4f} [{status}]\")\n",
    "\n",
    "# Req 2: Accuracy ~ F1\n",
    "print(\"\\n  [2] Accuracy ~ F1-Score (diff < 0.05)\")\n",
    "diff = abs(accuracy - macro_f1)\n",
    "status2 = \"PASS\" if diff < 0.05 else \"FAIL\"\n",
    "if diff >= 0.05:\n",
    "    all_pass = False\n",
    "print(f\"      Acc={accuracy:.4f}, F1={macro_f1:.4f}, Diff={diff:.4f} [{status2}]\")\n",
    "\n",
    "# Req 3: Class F1 similar\n",
    "print(\"\\n  [3] Class F1-Scores Similar (max diff < 0.15)\")\n",
    "f1_diff = max(f1) - min(f1)\n",
    "status3 = \"PASS\" if f1_diff < 0.15 else \"FAIL\"\n",
    "if f1_diff >= 0.15:\n",
    "    all_pass = False\n",
    "print(f\"      F1s: {[f'{x:.2f}' for x in f1]}, Max diff={f1_diff:.4f} [{status3}]\")\n",
    "\n",
    "# Req 4: Train-Val gap\n",
    "print(\"\\n  [4] Train-Val Gap < 15%\")\n",
    "train_val_gap = gap  # from earlier\n",
    "status4 = \"PASS\" if train_val_gap < 15 else \"FAIL\"\n",
    "if train_val_gap >= 15:\n",
    "    all_pass = False\n",
    "print(f\"      Gap={train_val_gap:.2f}% [{status4}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"  FINAL: {'ALL REQUIREMENTS PASSED!' if all_pass else 'SOME REQUIREMENTS NEED WORK'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 18. SAVE MODEL INFO\n",
    "# ============================================================\n",
    "\n",
    "model_info = {\n",
    "    'model_name': 'coconut_mite_3class_v9_robust',\n",
    "    'version': 'v9_robust',\n",
    "    'architecture': 'EfficientNetB0 + Fine-tuning',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'classes': class_names_ordered,\n",
    "    'input_size': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'dataset': {\n",
    "        'version': 'v4_clean',\n",
    "        'train': train_generator.samples,\n",
    "        'validation': val_generator.samples,\n",
    "        'test': test_generator.samples,\n",
    "        'data_leaks': 0,\n",
    "        'corrupted_files': 0\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(accuracy),\n",
    "        'macro_f1': float(macro_f1),\n",
    "        'per_class': [\n",
    "            {\n",
    "                'class': class_names_ordered[i],\n",
    "                'precision': float(precision[i]),\n",
    "                'recall': float(recall[i]),\n",
    "                'f1': float(f1[i]),\n",
    "                'support': int(support[i])\n",
    "            }\n",
    "            for i in range(NUM_CLASSES)\n",
    "        ],\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    },\n",
    "    'training': {\n",
    "        'total_epochs': len(history['accuracy']),\n",
    "        'best_epoch': int(best_epoch),\n",
    "        'training_time_minutes': float(total_training_time),\n",
    "        'train_val_gap': float(train_val_gap),\n",
    "        'loss_function': 'FocalLoss(gamma=2.0)',\n",
    "        'class_weights_used': True\n",
    "    },\n",
    "    'requirements_check': {\n",
    "        'pr_balanced': all(max(precision[i], recall[i], f1[i]) - min(precision[i], recall[i], f1[i]) < 0.10 for i in range(NUM_CLASSES)),\n",
    "        'acc_equals_f1': diff < 0.05,\n",
    "        'f1_similar': f1_diff < 0.15,\n",
    "        'gap_ok': train_val_gap < 15,\n",
    "        'all_pass': all_pass\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model info saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 19. FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"  V9 ROBUST MODEL - TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Dataset:    v4_clean ({train_generator.samples + val_generator.samples + test_generator.samples} images)\")\n",
    "print(f\"  Training:   {total_training_time:.1f} minutes\")\n",
    "print(f\"  Best Epoch: {best_epoch}\")\n",
    "print(f\"\\n  TEST RESULTS:\")\n",
    "print(f\"    Accuracy:   {accuracy*100:.2f}%\")\n",
    "print(f\"    Macro F1:   {macro_f1*100:.2f}%\")\n",
    "print(f\"\\n  Per-Class F1:\")\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    print(f\"    {cls}: {f1[i]*100:.2f}%\")\n",
    "print(f\"\\n  Train-Val Gap: {train_val_gap:.2f}%\")\n",
    "print(f\"\\n  Requirements: {'ALL PASS' if all_pass else 'NEEDS WORK'}\")\n",
    "print(\"\\n  Model saved to:\", MODEL_DIR)\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
