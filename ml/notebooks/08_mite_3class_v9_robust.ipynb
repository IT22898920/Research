{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut Mite Detection - 3-Class Model v9\n",
    "\n",
    "\n",
    "## Key Improvements from v8:\n",
    "- **Clean Dataset**: v4_clean with no data leaks or corrupted files\n",
    "- **Class Weights**: Computed from training data to handle imbalance\n",
    "- **Focal Loss**: Better handling of hard examples\n",
    "- **Honest Evaluation**: No tricks, real performance metrics\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model | EfficientNetB0 (Transfer Learning) |\n",
    "| Classes | coconut_mite, healthy, not_coconut |\n",
    "| Dataset | v4_clean (13,781 images, no leaks) |\n",
    "| Loss | Focal Loss with class weights |\n",
    "\n",
    "---\n",
    "**Author:** Research Team  \n",
    "**Date:** 2025-12-25  \n",
    "**Version:** v9 (Robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  ENVIRONMENT\n",
      "============================================================\n",
      "  TensorFlow: 2.20.0\n",
      "  GPU: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  ENVIRONMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  TensorFlow: {tf.__version__}\")\n",
    "print(f\"  GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  CONFIGURATION - V9 ROBUST\n",
      "============================================================\n",
      "\n",
      "  Dataset: d:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\data\\raw\\pest_mite\\dataset_v4_clean\n",
      "  Model:   d:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\models\\coconut_mite_v9\n",
      "\n",
      "  Hyperparameters:\n",
      "    Image Size:    224x224\n",
      "    Batch Size:    32\n",
      "    Max Epochs:    100\n",
      "    Learning Rate: 0.0001\n",
      "    Dropout:       0.5\n",
      "    L2 Reg:        0.01\n",
      "    Early Stop:    10 epochs\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. CONFIGURATION\n",
    "# ============================================================\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'raw', 'pest_mite', 'dataset_v4_clean')  # CLEAN DATASET!\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models', 'coconut_mite_v9')\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'validation')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "# Hyperparameters - Conservative for honest evaluation\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # More epochs, rely on early stopping\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_REG = 0.01\n",
    "PATIENCE = 10  # More patience for better convergence\n",
    "\n",
    "CLASS_NAMES = ['coconut_mite', 'healthy', 'not_coconut']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  CONFIGURATION - V9 ROBUST\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Dataset: {DATA_DIR}\")\n",
    "print(f\"  Model:   {MODEL_DIR}\")\n",
    "print(f\"\\n  Hyperparameters:\")\n",
    "print(f\"    Image Size:    {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"    Batch Size:    {BATCH_SIZE}\")\n",
    "print(f\"    Max Epochs:    {EPOCHS}\")\n",
    "print(f\"    Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"    Dropout:       {DROPOUT_RATE}\")\n",
    "print(f\"    L2 Reg:        {L2_REG}\")\n",
    "print(f\"    Early Stop:    {PATIENCE} epochs\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  DATASET v4_clean (NO DATA LEAKS!)\n",
      "============================================================\n",
      "\n",
      "  Split            mite    healthy     not_coco    Total\n",
      "  --------------------------------------------------\n",
      "  train            4739       4204         3985    12928\n",
      "  validation         99         89          291      479\n",
      "  test              100         89          185      374\n",
      "  --------------------------------------------------\n",
      "  TOTAL            4938       4382         4461    13781\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. DATASET VERIFICATION\n",
    "# ============================================================\n",
    "def count_images(directory):\n",
    "    counts = {}\n",
    "    for cls in CLASS_NAMES:\n",
    "        cls_dir = os.path.join(directory, cls)\n",
    "        if os.path.exists(cls_dir):\n",
    "            counts[cls] = len([f for f in os.listdir(cls_dir) \n",
    "                              if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        else:\n",
    "            counts[cls] = 0\n",
    "    return counts\n",
    "\n",
    "train_counts = count_images(TRAIN_DIR)\n",
    "val_counts = count_images(VAL_DIR)\n",
    "test_counts = count_images(TEST_DIR)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  DATASET v4_clean (NO DATA LEAKS!)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  {'Split':<12} {'mite':>8} {'healthy':>10} {'not_coco':>12} {'Total':>8}\")\n",
    "print(\"  \" + \"-\" * 50)\n",
    "\n",
    "for split, counts in [('train', train_counts), ('validation', val_counts), ('test', test_counts)]:\n",
    "    total = sum(counts.values())\n",
    "    print(f\"  {split:<12} {counts['coconut_mite']:>8} {counts['healthy']:>10} {counts['not_coconut']:>12} {total:>8}\")\n",
    "\n",
    "total_all = sum(train_counts.values()) + sum(val_counts.values()) + sum(test_counts.values())\n",
    "print(\"  \" + \"-\" * 50)\n",
    "print(f\"  {'TOTAL':<12} {train_counts['coconut_mite']+val_counts['coconut_mite']+test_counts['coconut_mite']:>8} \"\n",
    "      f\"{train_counts['healthy']+val_counts['healthy']+test_counts['healthy']:>10} \"\n",
    "      f\"{train_counts['not_coconut']+val_counts['not_coconut']+test_counts['not_coconut']:>12} {total_all:>8}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  CLASS WEIGHTS (for balanced learning)\n",
      "============================================================\n",
      "  coconut_mite: 0.9093\n",
      "  healthy: 1.0251\n",
      "  not_coconut: 1.0814\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. COMPUTE CLASS WEIGHTS\n",
    "# ============================================================\n",
    "# This helps the model pay equal attention to all classes\n",
    "\n",
    "train_labels = []\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    train_labels.extend([i] * train_counts[cls])\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "\n",
    "class_weights = {i: w for i, w in enumerate(class_weights_array)}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  CLASS WEIGHTS (for balanced learning)\")\n",
    "print(\"=\" * 60)\n",
    "for i, cls in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {cls}: {class_weights[i]:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12928 images belonging to 3 classes.\n",
      "Found 479 images belonging to 3 classes.\n",
      "Found 374 images belonging to 3 classes.\n",
      "============================================================\n",
      "  DATA GENERATORS\n",
      "============================================================\n",
      "  Train: 12928 samples, 404 batches\n",
      "  Val:   479 samples\n",
      "  Test:  374 samples\n",
      "\n",
      "  Class indices: {'coconut_mite': 0, 'healthy': 1, 'not_coconut': 2}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. DATA GENERATORS\n",
    "# ============================================================\n",
    "\n",
    "# Moderate augmentation - not too aggressive\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  DATA GENERATORS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Train: {train_generator.samples} samples, {len(train_generator)} batches\")\n",
    "print(f\"  Val:   {val_generator.samples} samples\")\n",
    "print(f\"  Test:  {test_generator.samples} samples\")\n",
    "print(f\"\\n  Class indices: {train_generator.class_indices}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss defined with gamma=2.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. FOCAL LOSS (Better for imbalanced/hard examples)\n",
    "# ============================================================\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss for handling class imbalance and hard examples.\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Cross entropy\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Focal weight\n",
    "        weight = tf.pow(1 - y_pred, self.gamma) * y_true\n",
    "        \n",
    "        # Apply alpha if provided\n",
    "        if self.alpha is not None:\n",
    "            weight = weight * self.alpha\n",
    "        \n",
    "        focal_loss = weight * ce\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "print(\"Focal Loss defined with gamma=2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  MODEL ARCHITECTURE\n",
      "============================================================\n",
      "  Total params: 4,227,110\n",
      "  Trainable: 174,979\n",
      "  Non-trainable: 4,052,131\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. BUILD MODEL\n",
    "# ============================================================\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build EfficientNetB0 for 3-class classification.\"\"\"\n",
    "    \n",
    "    # Base model\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Custom head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(L2_REG))(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    # Compile with focal loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=FocalLoss(gamma=2.0),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "model, base_model = build_model()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total params: {model.count_params():,}\")\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Non-trainable: {model.count_params() - trainable:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. CALLBACKS\n",
    "# ============================================================\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "print(\"Callbacks configured: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  PHASE 1: Training with frozen EfficientNetB0\n",
      "============================================================\n",
      "Epoch 1/30\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3456 - loss: 3.7272\n",
      "Epoch 1: val_accuracy improved from None to 0.55115, saving model to d:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\models\\coconut_mite_v9\\best_model.keras\n",
      "\u001b[1m404/404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m582s\u001b[0m 1s/step - accuracy: 0.3424 - loss: 3.5097 - val_accuracy: 0.5511 - val_loss: 3.1150 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m260/404\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 1s/step - accuracy: 0.3555 - loss: 3.0684"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m      9\u001b[39m start_time = datetime.now()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m history1 = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Initial training\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m phase1_time = (datetime.now() - start_time).total_seconds() / \u001b[32m60\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPhase 1 completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase1_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. PHASE 1: TRAIN WITH FROZEN BASE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  PHASE 1: Training with frozen EfficientNetB0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Initial training\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nPhase 1 completed in {phase1_time:.1f} minutes\")\n",
    "print(f\"Best val_accuracy: {max(history1.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. PHASE 2: FINE-TUNE TOP LAYERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  PHASE 2: Fine-tuning top layers of EfficientNetB0\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze top 20 layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE / 10),\n",
    "    loss=FocalLoss(gamma=2.0),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"Trainable params after unfreezing: {trainable:,}\")\n",
    "\n",
    "# Continue training\n",
    "start_time = datetime.now()\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase2_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nPhase 2 completed in {phase2_time:.1f} minutes\")\n",
    "print(f\"Best val_accuracy: {max(history2.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. COMBINE TRAINING HISTORY\n",
    "# ============================================================\n",
    "\n",
    "history = {\n",
    "    'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n",
    "    'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy'],\n",
    "    'loss': history1.history['loss'] + history2.history['loss'],\n",
    "    'val_loss': history1.history['val_loss'] + history2.history['val_loss']\n",
    "}\n",
    "\n",
    "total_training_time = phase1_time + phase2_time\n",
    "\n",
    "# Save history\n",
    "with open(os.path.join(MODEL_DIR, 'training_history.json'), 'w') as f:\n",
    "    json.dump({k: [float(v) for v in vals] for k, vals in history.items()}, f, indent=2)\n",
    "\n",
    "print(f\"Total training time: {total_training_time:.1f} minutes\")\n",
    "print(f\"Total epochs: {len(history['accuracy'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12. PLOT TRAINING HISTORY\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, [x*100 for x in history['accuracy']], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, [x*100 for x in history['val_accuracy']], 'r-', label='Validation', linewidth=2)\n",
    "axes[0].axvline(x=len(history1.history['accuracy']), color='green', linestyle='--', alpha=0.5, label='Fine-tune start')\n",
    "\n",
    "best_epoch = np.argmax(history['val_accuracy']) + 1\n",
    "best_val_acc = max(history['val_accuracy']) * 100\n",
    "axes[0].scatter([best_epoch], [best_val_acc], color='green', s=100, zorder=5)\n",
    "axes[0].annotate(f'Best: {best_val_acc:.1f}%', xy=(best_epoch, best_val_acc), \n",
    "                 xytext=(best_epoch+2, best_val_acc-3), fontsize=10)\n",
    "\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "axes[1].axvline(x=len(history1.history['loss']), color='green', linestyle='--', alpha=0.5, label='Fine-tune start')\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate gap\n",
    "final_train = history['accuracy'][-1] * 100\n",
    "final_val = history['val_accuracy'][-1] * 100\n",
    "gap = abs(final_train - final_val)\n",
    "print(f\"\\nFinal Train: {final_train:.2f}%, Val: {final_val:.2f}%, Gap: {gap:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13. LOAD BEST MODEL & EVALUATE ON TEST SET\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  TEST SET EVALUATION (HONEST METRICS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(\n",
    "    os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    custom_objects={'FocalLoss': FocalLoss}\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "test_generator.reset()\n",
    "y_probs = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Class names in correct order\n",
    "class_names_ordered = list(test_generator.class_indices.keys())\n",
    "\n",
    "print(f\"\\nTest samples: {len(y_true)}\")\n",
    "print(f\"Classes: {class_names_ordered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 14. CLASSIFICATION REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_ordered, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 15. DETAILED METRICS\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  DETAILED METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n  Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"  Macro F1-Score:   {macro_f1*100:.2f}%\")\n",
    "\n",
    "print(\"\\n  Per-Class:\")\n",
    "print(\"  \" + \"-\"*50)\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    print(f\"  {cls}:\")\n",
    "    print(f\"    Precision: {precision[i]*100:.2f}%\")\n",
    "    print(f\"    Recall:    {recall[i]*100:.2f}%\")\n",
    "    print(f\"    F1-Score:  {f1[i]*100:.2f}%\")\n",
    "    print(f\"    Support:   {support[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 16. CONFUSION MATRIX\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "# Normalized\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 17. REQUIREMENTS CHECK\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  UTHPALA MISS REQUIREMENTS CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_pass = True\n",
    "\n",
    "# Req 1: P/R/F1 balanced per class\n",
    "print(\"\\n  [1] P/R/F1 Balanced per Class (gap < 0.10)\")\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    gap = max(precision[i], recall[i], f1[i]) - min(precision[i], recall[i], f1[i])\n",
    "    status = \"PASS\" if gap < 0.10 else \"FAIL\"\n",
    "    if gap >= 0.10:\n",
    "        all_pass = False\n",
    "    print(f\"      {cls}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f} | Gap={gap:.4f} [{status}]\")\n",
    "\n",
    "# Req 2: Accuracy ~ F1\n",
    "print(\"\\n  [2] Accuracy ~ F1-Score (diff < 0.05)\")\n",
    "diff = abs(accuracy - macro_f1)\n",
    "status2 = \"PASS\" if diff < 0.05 else \"FAIL\"\n",
    "if diff >= 0.05:\n",
    "    all_pass = False\n",
    "print(f\"      Acc={accuracy:.4f}, F1={macro_f1:.4f}, Diff={diff:.4f} [{status2}]\")\n",
    "\n",
    "# Req 3: Class F1 similar\n",
    "print(\"\\n  [3] Class F1-Scores Similar (max diff < 0.15)\")\n",
    "f1_diff = max(f1) - min(f1)\n",
    "status3 = \"PASS\" if f1_diff < 0.15 else \"FAIL\"\n",
    "if f1_diff >= 0.15:\n",
    "    all_pass = False\n",
    "print(f\"      F1s: {[f'{x:.2f}' for x in f1]}, Max diff={f1_diff:.4f} [{status3}]\")\n",
    "\n",
    "# Req 4: Train-Val gap\n",
    "print(\"\\n  [4] Train-Val Gap < 15%\")\n",
    "train_val_gap = gap  # from earlier\n",
    "status4 = \"PASS\" if train_val_gap < 15 else \"FAIL\"\n",
    "if train_val_gap >= 15:\n",
    "    all_pass = False\n",
    "print(f\"      Gap={train_val_gap:.2f}% [{status4}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"  FINAL: {'ALL REQUIREMENTS PASSED!' if all_pass else 'SOME REQUIREMENTS NEED WORK'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 18. SAVE MODEL INFO\n",
    "# ============================================================\n",
    "\n",
    "model_info = {\n",
    "    'model_name': 'coconut_mite_3class_v9_robust',\n",
    "    'version': 'v9_robust',\n",
    "    'architecture': 'EfficientNetB0 + Fine-tuning',\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'classes': class_names_ordered,\n",
    "    'input_size': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'dataset': {\n",
    "        'version': 'v4_clean',\n",
    "        'train': train_generator.samples,\n",
    "        'validation': val_generator.samples,\n",
    "        'test': test_generator.samples,\n",
    "        'data_leaks': 0,\n",
    "        'corrupted_files': 0\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(accuracy),\n",
    "        'macro_f1': float(macro_f1),\n",
    "        'per_class': [\n",
    "            {\n",
    "                'class': class_names_ordered[i],\n",
    "                'precision': float(precision[i]),\n",
    "                'recall': float(recall[i]),\n",
    "                'f1': float(f1[i]),\n",
    "                'support': int(support[i])\n",
    "            }\n",
    "            for i in range(NUM_CLASSES)\n",
    "        ],\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    },\n",
    "    'training': {\n",
    "        'total_epochs': len(history['accuracy']),\n",
    "        'best_epoch': int(best_epoch),\n",
    "        'training_time_minutes': float(total_training_time),\n",
    "        'train_val_gap': float(train_val_gap),\n",
    "        'loss_function': 'FocalLoss(gamma=2.0)',\n",
    "        'class_weights_used': True\n",
    "    },\n",
    "    'requirements_check': {\n",
    "        'pr_balanced': all(max(precision[i], recall[i], f1[i]) - min(precision[i], recall[i], f1[i]) < 0.10 for i in range(NUM_CLASSES)),\n",
    "        'acc_equals_f1': diff < 0.05,\n",
    "        'f1_similar': f1_diff < 0.15,\n",
    "        'gap_ok': train_val_gap < 15,\n",
    "        'all_pass': all_pass\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"Model info saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 19. FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 60)\n",
    "print(\"  V9 ROBUST MODEL - TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Dataset:    v4_clean ({train_generator.samples + val_generator.samples + test_generator.samples} images)\")\n",
    "print(f\"  Training:   {total_training_time:.1f} minutes\")\n",
    "print(f\"  Best Epoch: {best_epoch}\")\n",
    "print(f\"\\n  TEST RESULTS:\")\n",
    "print(f\"    Accuracy:   {accuracy*100:.2f}%\")\n",
    "print(f\"    Macro F1:   {macro_f1*100:.2f}%\")\n",
    "print(f\"\\n  Per-Class F1:\")\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    print(f\"    {cls}: {f1[i]*100:.2f}%\")\n",
    "print(f\"\\n  Train-Val Gap: {train_val_gap:.2f}%\")\n",
    "print(f\"\\n  Requirements: {'ALL PASS' if all_pass else 'NEEDS WORK'}\")\n",
    "print(\"\\n  Model saved to:\", MODEL_DIR)\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
