{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut Tree Branch Health Detection Model v1\n",
    "\n",
    "This notebook trains a model to detect healthy vs unhealthy coconut tree branches.\n",
    "\n",
    "**Model Configuration:**\n",
    "- **Architecture:** MobileNetV2 (Transfer Learning)\n",
    "- **Loss Function:** Focal Loss (gamma=2.0) for handling class imbalance\n",
    "- **Training Strategy:** 2-phase training (frozen base â†’ fine-tuning)\n",
    "- **Classes:** 2 (healthy, unhealthy)\n",
    "\n",
    "**Goals:**\n",
    "- Detect unhealthy coconut tree branches\n",
    "- Provide unhealthy percentage\n",
    "- High accuracy and balanced metrics\n",
    "- Avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import random\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_DIR = os.path.abspath('..')\n",
    "RAW_DATA_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "DATASET_DIR = os.path.join(RAW_DATA_DIR, 'coconut_branch_health', 'dataset')\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models', 'coconut_branch_health_v1')\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "PHASE1_EPOCHS = 20  # Frozen base\n",
    "PHASE2_EPOCHS = 15  # Fine-tuning\n",
    "LEARNING_RATE_PHASE1 = 1e-3\n",
    "LEARNING_RATE_PHASE2 = 1e-4\n",
    "\n",
    "# Classes\n",
    "class_names = ['healthy', 'unhealthy']\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Dataset Directory: {DATASET_DIR}\")\n",
    "print(f\"Model Directory: {MODEL_DIR}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Proper Dataset Structure\n",
    "\n",
    "Reorganize data from separate folders into ImageDataGenerator-compatible structure:\n",
    "```\n",
    "dataset/\n",
    "  train/\n",
    "    healthy/\n",
    "    unhealthy/\n",
    "  val/\n",
    "    healthy/\n",
    "    unhealthy/\n",
    "  test/\n",
    "    healthy/\n",
    "    unhealthy/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source directories\n",
    "healthy_src = os.path.join(RAW_DATA_DIR, 'healthy-leaves')\n",
    "unhealthy_src = os.path.join(RAW_DATA_DIR, 'coconut_tree_branch_Health')\n",
    "\n",
    "print(f\"Healthy source: {healthy_src}\")\n",
    "print(f\"Unhealthy source: {unhealthy_src}\")\n",
    "\n",
    "# Create dataset structure\n",
    "for split in ['training', 'validation', 'test']:\n",
    "    for cls in ['healthy', 'unhealthy']:\n",
    "        # Map to correct folder names\n",
    "        if split == 'training':\n",
    "            dest_split = 'train'\n",
    "        elif split == 'validation':\n",
    "            dest_split = 'val'\n",
    "        else:\n",
    "            dest_split = 'test'\n",
    "        \n",
    "        dest_dir = os.path.join(DATASET_DIR, dest_split, cls)\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        \n",
    "        # Source directory\n",
    "        if cls == 'healthy':\n",
    "            src_dir = os.path.join(healthy_src, split)\n",
    "        else:\n",
    "            src_dir = os.path.join(unhealthy_src, split)\n",
    "        \n",
    "        # Copy files if destination is empty\n",
    "        if len(os.listdir(dest_dir)) == 0:\n",
    "            files = [f for f in os.listdir(src_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            for f in files:\n",
    "                shutil.copy2(os.path.join(src_dir, f), os.path.join(dest_dir, f))\n",
    "            print(f\"Copied {len(files)} files to {dest_split}/{cls}/\")\n",
    "        else:\n",
    "            print(f\"{dest_split}/{cls}/ already has {len(os.listdir(dest_dir))} files\")\n",
    "\n",
    "print(\"\\nDataset structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each split and class\n",
    "data_summary = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_path = os.path.join(DATASET_DIR, split)\n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    split_total = 0\n",
    "    for cls in class_names:\n",
    "        cls_path = os.path.join(split_path, cls)\n",
    "        count = len([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        split_total += count\n",
    "        print(f\"  {cls:<15} {count:>6} images\")\n",
    "        \n",
    "        if split not in data_summary:\n",
    "            data_summary[split] = {}\n",
    "        data_summary[split][cls] = count\n",
    "    \n",
    "    print(f\"  {'TOTAL':<15} {split_total:>6} images\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('Class Distribution Across Splits', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, split in enumerate(['train', 'val', 'test']):\n",
    "    counts = [data_summary[split][cls] for cls in class_names]\n",
    "    axes[idx].bar(class_names, counts, color=['green', 'red'])\n",
    "    axes[idx].set_title(f'{split.upper()} Split', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Number of Images')\n",
    "    axes[idx].set_xlabel('Class')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, count in enumerate(counts):\n",
    "        axes[idx].text(i, count, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'class_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "train_healthy = data_summary['train']['healthy']\n",
    "train_unhealthy = data_summary['train']['unhealthy']\n",
    "imbalance_ratio = max(train_healthy, train_unhealthy) / min(train_healthy, train_unhealthy)\n",
    "print(f\"\\nClass imbalance ratio (train): {imbalance_ratio:.2f}\")\n",
    "if imbalance_ratio > 1.5:\n",
    "    print(\"âš  Class imbalance detected - Focal Loss will help!\")\n",
    "else:\n",
    "    print(\"âœ“ Classes are relatively balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Show sample images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "fig.suptitle('Sample Images from Training Set', fontsize=14, fontweight='bold')\n",
    "\n",
    "for row, cls in enumerate(class_names):\n",
    "    cls_dir = os.path.join(DATASET_DIR, 'train', cls)\n",
    "    images_list = [f for f in os.listdir(cls_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Get 5 random images\n",
    "    sample_imgs = random.sample(images_list, min(5, len(images_list)))\n",
    "    \n",
    "    for col, img_name in enumerate(sample_imgs):\n",
    "        img_path = os.path.join(cls_dir, img_name)\n",
    "        img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Add class label on the left\n",
    "    axes[row, 0].text(-0.3, 0.5, cls.upper(), transform=axes[row, 0].transAxes,\n",
    "                      fontsize=12, fontweight='bold', va='center', ha='right',\n",
    "                      color='green' if cls == 'healthy' else 'red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'sample_images.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Generators with Augmentation\n",
    "\n",
    "**Important:** Using ImageDataGenerator with proper validation split prevents data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation and test generators (no augmentation, only rescaling)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(DATASET_DIR, 'train'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    classes=class_names,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(DATASET_DIR, 'val'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    classes=class_names,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(DATASET_DIR, 'test'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    classes=class_names,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Test samples: {test_generator.samples}\")\n",
    "print(f\"Class indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Model with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Build MobileNetV2 model with transfer learning\"\"\"\n",
    "    \n",
    "    # Load pre-trained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_model()\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nBase model layers: {len(base_model.layers)}\")\n",
    "print(f\"Total model layers: {len(model.layers)}\")\n",
    "print(f\"Trainable layers: {sum([layer.trainable for layer in model.layers])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Focal Loss\n",
    "\n",
    "Focal Loss helps with class imbalance by focusing on hard-to-classify examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter (higher = more focus on hard examples)\n",
    "        alpha: Balancing parameter\n",
    "    \"\"\"\n",
    "    def focal_loss_fn(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Cross entropy\n",
    "        cross_entropy = -y_true * tf.keras.backend.log(y_pred)\n",
    "        \n",
    "        # Focal weight\n",
    "        focal_weight = tf.keras.backend.pow(1.0 - y_pred, gamma)\n",
    "        \n",
    "        # Focal loss\n",
    "        focal_loss = alpha * focal_weight * cross_entropy\n",
    "        \n",
    "        return tf.keras.backend.sum(focal_loss, axis=-1)\n",
    "    \n",
    "    return focal_loss_fn\n",
    "\n",
    "print(\"Focal Loss function defined!\")\n",
    "print(f\"  Gamma: 2.0 (focus on hard examples)\")\n",
    "print(f\"  Alpha: 0.25 (class balancing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phase 1: Train with Frozen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for Phase 1\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_PHASE1),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_phase1 = ModelCheckpoint(\n",
    "    os.path.join(MODEL_DIR, 'phase1_best.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop_phase1 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_phase1 = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: Training with Frozen Base Model\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Learning Rate: {LEARNING_RATE_PHASE1}\")\n",
    "print(f\"Epochs: {PHASE1_EPOCHS}\")\n",
    "print(f\"Base Model Trainable: {base_model.trainable}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Phase 1\n",
    "start_time = time.time()\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=PHASE1_EPOCHS,\n",
    "    callbacks=[checkpoint_phase1, early_stop_phase1, reduce_lr_phase1],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = (time.time() - start_time) / 60\n",
    "\n",
    "print(f\"\\nPhase 1 completed in {phase1_time:.1f} minutes\")\n",
    "print(f\"Best validation accuracy: {max(history_phase1.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Phase 2: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers, only fine-tune last layers\n",
    "fine_tune_at = len(base_model.layers) - 30\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Total base model layers: {len(base_model.layers)}\")\n",
    "print(f\"Fine-tuning from layer: {fine_tune_at}\")\n",
    "print(f\"Trainable layers: {sum([layer.trainable for layer in model.layers])}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_PHASE2),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for Phase 2\n",
    "checkpoint_phase2 = ModelCheckpoint(\n",
    "    os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop_phase2 = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_phase2 = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-8,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: Fine-tuning\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Learning Rate: {LEARNING_RATE_PHASE2}\")\n",
    "print(f\"Epochs: {PHASE2_EPOCHS}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Phase 2\n",
    "start_time = time.time()\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=PHASE2_EPOCHS,\n",
    "    callbacks=[checkpoint_phase2, early_stop_phase2, reduce_lr_phase2],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase2_time = (time.time() - start_time) / 60\n",
    "total_time = phase1_time + phase2_time\n",
    "\n",
    "print(f\"\\nPhase 2 completed in {phase2_time:.1f} minutes\")\n",
    "print(f\"Total training time: {total_time:.1f} minutes\")\n",
    "print(f\"Best validation accuracy: {max(history_phase2.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "history_combined = {\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
    "}\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Training History - Coconut Branch Health Model v1', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history_combined['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history_combined['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].axvline(x=len(history_phase1.history['accuracy'])-1, color='red', linestyle='--', label='Fine-tuning starts', alpha=0.7)\n",
    "axes[0].set_title('Model Accuracy', fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history_combined['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history_combined['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].axvline(x=len(history_phase1.history['loss'])-1, color='red', linestyle='--', label='Fine-tuning starts', alpha=0.7)\n",
    "axes[1].set_title('Model Loss', fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "final_train_acc = history_combined['accuracy'][-1]\n",
    "final_val_acc = history_combined['val_accuracy'][-1]\n",
    "acc_gap = abs(final_train_acc - final_val_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERFITTING CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final Train Accuracy: {final_train_acc*100:.2f}%\")\n",
    "print(f\"Final Val Accuracy: {final_val_acc*100:.2f}%\")\n",
    "print(f\"Accuracy Gap: {acc_gap*100:.2f}%\")\n",
    "\n",
    "if acc_gap < 0.05:\n",
    "    print(\"âœ“ No significant overfitting detected!\")\n",
    "elif acc_gap < 0.10:\n",
    "    print(\"âš  Minor overfitting - acceptable\")\n",
    "else:\n",
    "    print(\"âš âš  Overfitting detected - consider more regularization\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load Best Model and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = keras.models.load_model(\n",
    "    os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    custom_objects={'focal_loss_fn': focal_loss(gamma=2.0, alpha=0.25)}\n",
    ")\n",
    "\n",
    "print(\"Best model loaded!\")\n",
    "\n",
    "# Make predictions on test set\n",
    "test_generator.reset()\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Get true labels and predicted labels\n",
    "y_true = test_generator.classes\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = np.mean(y_true == y_pred)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Detailed Metrics - Class-wise Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETAILED CLASS-WISE METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Class':<15} {'Precision':>12} {'Recall':>12} {'F1-Score':>12} {'Support':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"{cls:<15} {precision[i]*100:>11.2f}% {recall[i]*100:>11.2f}% {f1[i]*100:>11.2f}% {support[i]:>10}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Macro Average':<15} {macro_precision*100:>11.2f}% {macro_recall*100:>11.2f}% {macro_f1*100:>11.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check metric balance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METRIC BALANCE CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, cls in enumerate(class_names):\n",
    "    p, r, f = precision[i], recall[i], f1[i]\n",
    "    max_diff = max(abs(p-r), abs(p-f), abs(r-f))\n",
    "    print(f\"\\n{cls.upper()}:\")\n",
    "    print(f\"  Precision: {p*100:.2f}%\")\n",
    "    print(f\"  Recall: {r*100:.2f}%\")\n",
    "    print(f\"  F1-Score: {f*100:.2f}%\")\n",
    "    print(f\"  Max difference: {max_diff*100:.2f}%\")\n",
    "    \n",
    "    if max_diff < 0.05:\n",
    "        print(f\"  âœ“ Well balanced!\")\n",
    "    elif max_diff < 0.10:\n",
    "        print(f\"  âš  Acceptable balance\")\n",
    "    else:\n",
    "        print(f\"  âš âš  Imbalanced - one metric is weaker\")\n",
    "\n",
    "# Check if accuracy is close to F1\n",
    "acc_f1_diff = abs(test_accuracy - macro_f1)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Macro F1: {macro_f1*100:.2f}%\")\n",
    "print(f\"Difference: {acc_f1_diff*100:.2f}%\")\n",
    "\n",
    "if acc_f1_diff < 0.03:\n",
    "    print(\"âœ“ Accuracy and F1 are very close!\")\n",
    "elif acc_f1_diff < 0.05:\n",
    "    print(\"âœ“ Accuracy and F1 are reasonably close\")\n",
    "else:\n",
    "    print(\"âš  Accuracy and F1 have notable difference\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Confusion Matrix - Coconut Branch Health Model v1', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[0],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "\n",
    "# Percentages\n",
    "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=axes[1],\n",
    "            cbar_kws={'label': 'Percentage (%)'})\n",
    "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix details\n",
    "print(\"\\nConfusion Matrix Details:\")\n",
    "print(\"-\"*50)\n",
    "for i, true_cls in enumerate(class_names):\n",
    "    for j, pred_cls in enumerate(class_names):\n",
    "        count = cm[i, j]\n",
    "        pct = cm_pct[i, j]\n",
    "        if i == j:\n",
    "            print(f\"âœ“ {true_cls} correctly classified: {count} ({pct:.1f}%)\")\n",
    "        elif count > 0:\n",
    "            print(f\"âœ— {true_cls} misclassified as {pred_cls}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full classification report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames\n",
    "filenames = test_generator.filenames\n",
    "\n",
    "# Find correct and wrong predictions\n",
    "correct_idx = [i for i in range(len(y_true)) if y_true[i] == y_pred[i]]\n",
    "wrong_idx = [i for i in range(len(y_true)) if y_true[i] != y_pred[i]]\n",
    "\n",
    "print(f\"Total: {len(y_true)} | Correct: {len(correct_idx)} | Wrong: {len(wrong_idx)}\")\n",
    "\n",
    "# Plot correct predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 7))\n",
    "fig.suptitle('CORRECT Predictions (Sample)', fontsize=14, fontweight='bold', color='green')\n",
    "\n",
    "sample_correct = random.sample(correct_idx, min(10, len(correct_idx)))\n",
    "for idx, i in enumerate(sample_correct):\n",
    "    row, col = idx // 5, idx % 5\n",
    "    img_path = os.path.join(DATASET_DIR, 'test', filenames[i])\n",
    "    img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].axis('off')\n",
    "    true_label = class_names[y_true[i]]\n",
    "    pred_label = class_names[y_pred[i]]\n",
    "    confidence = predictions[i][y_pred[i]] * 100\n",
    "    axes[row, col].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)', \n",
    "                             fontsize=9, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'correct_predictions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot wrong predictions if any\n",
    "if len(wrong_idx) > 0:\n",
    "    n_wrong = min(10, len(wrong_idx))\n",
    "    rows = (n_wrong + 4) // 5\n",
    "    fig, axes = plt.subplots(rows, 5, figsize=(15, 3.5*rows))\n",
    "    fig.suptitle(f'WRONG Predictions (All {len(wrong_idx)})', fontsize=14, fontweight='bold', color='red')\n",
    "    \n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, i in enumerate(wrong_idx[:n_wrong]):\n",
    "        row, col = idx // 5, idx % 5\n",
    "        img_path = os.path.join(DATASET_DIR, 'test', filenames[i])\n",
    "        img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "        true_label = class_names[y_true[i]]\n",
    "        pred_label = class_names[y_pred[i]]\n",
    "        confidence = predictions[i][y_pred[i]] * 100\n",
    "        axes[row, col].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)', \n",
    "                                fontsize=9, color='red')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_wrong, rows * 5):\n",
    "        row, col = idx // 5, idx % 5\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_DIR, 'wrong_predictions.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ Perfect! No wrong predictions!\")\n",
    "\n",
    "print(f\"\\nImages saved to {MODEL_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Save Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model info dictionary\n",
    "model_info = {\n",
    "    'model_name': 'coconut_branch_health_v1',\n",
    "    'architecture': 'MobileNetV2',\n",
    "    'classes': class_names,\n",
    "    'num_classes': len(class_names),\n",
    "    'input_shape': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'training': {\n",
    "        'phase1_epochs': PHASE1_EPOCHS,\n",
    "        'phase2_epochs': PHASE2_EPOCHS,\n",
    "        'total_epochs': PHASE1_EPOCHS + PHASE2_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate_phase1': LEARNING_RATE_PHASE1,\n",
    "        'learning_rate_phase2': LEARNING_RATE_PHASE2,\n",
    "        'loss_function': 'Focal Loss (gamma=2.0, alpha=0.25)',\n",
    "        'optimizer': 'Adam',\n",
    "        'training_time_minutes': round(total_time, 1),\n",
    "        'final_train_accuracy': float(final_train_acc),\n",
    "        'final_val_accuracy': float(final_val_acc)\n",
    "    },\n",
    "    'data': {\n",
    "        'train_samples': train_generator.samples,\n",
    "        'val_samples': val_generator.samples,\n",
    "        'test_samples': test_generator.samples,\n",
    "        'train_healthy': data_summary['train']['healthy'],\n",
    "        'train_unhealthy': data_summary['train']['unhealthy'],\n",
    "        'class_imbalance_ratio': float(imbalance_ratio)\n",
    "    },\n",
    "    'test_performance': {\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'macro_precision': float(macro_precision),\n",
    "        'macro_recall': float(macro_recall),\n",
    "        'macro_f1': float(macro_f1),\n",
    "        'healthy_precision': float(precision[0]),\n",
    "        'healthy_recall': float(recall[0]),\n",
    "        'healthy_f1': float(f1[0]),\n",
    "        'unhealthy_precision': float(precision[1]),\n",
    "        'unhealthy_recall': float(recall[1]),\n",
    "        'unhealthy_f1': float(f1[1])\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'rotation_range': 30,\n",
    "        'width_shift_range': 0.2,\n",
    "        'height_shift_range': 0.2,\n",
    "        'horizontal_flip': True,\n",
    "        'vertical_flip': True,\n",
    "        'zoom_range': 0.2,\n",
    "        'brightness_range': [0.8, 1.2]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model info\n",
    "model_info_path = os.path.join(MODEL_DIR, 'model_info.json')\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"Model information saved to: {model_info_path}\")\n",
    "print(\"\\nModel files created:\")\n",
    "print(f\"  - best_model.keras\")\n",
    "print(f\"  - model_info.json\")\n",
    "print(f\"  - training_history.png\")\n",
    "print(f\"  - confusion_matrix.png\")\n",
    "print(f\"  - class_distribution.png\")\n",
    "print(f\"  - sample_images.png\")\n",
    "print(f\"  - correct_predictions.png\")\n",
    "if len(wrong_idx) > 0:\n",
    "    print(f\"  - wrong_predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COCONUT BRANCH HEALTH MODEL v1 - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"  Model Information:\")\n",
    "print(\"  \" + \"-\"*60)\n",
    "print(f\"  Name:                    Coconut Branch Health Detection Model v1\")\n",
    "print(f\"  Architecture:            MobileNetV2 (Transfer Learning)\")\n",
    "print(f\"  Loss Function:           Focal Loss (gamma=2.0)\")\n",
    "print(f\"  Input Size:              {IMG_SIZE}x{IMG_SIZE}x3\")\n",
    "print(f\"  Classes:                 {class_names}\")\n",
    "print()\n",
    "print(\"  Training Summary:\")\n",
    "print(\"  \" + \"-\"*60)\n",
    "print(f\"  Phase 1 (Frozen):        {PHASE1_EPOCHS} epochs, LR={LEARNING_RATE_PHASE1}\")\n",
    "print(f\"  Phase 2 (Fine-tune):     {PHASE2_EPOCHS} epochs, LR={LEARNING_RATE_PHASE2}\")\n",
    "print(f\"  Total Training Time:     {total_time:.1f} minutes\")\n",
    "print(f\"  Final Train Accuracy:    {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Final Val Accuracy:      {final_val_acc*100:.2f}%\")\n",
    "print()\n",
    "print(\"  Test Performance:\")\n",
    "print(\"  \" + \"-\"*60)\n",
    "print(f\"  Test Accuracy:           {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Macro Precision:         {macro_precision*100:.2f}%\")\n",
    "print(f\"  Macro Recall:            {macro_recall*100:.2f}%\")\n",
    "print(f\"  Macro F1-Score:          {macro_f1*100:.2f}%\")\n",
    "print()\n",
    "print(\"  Class-wise Performance:\")\n",
    "print(\"  \" + \"-\"*60)\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"  {cls.upper():12} P={precision[i]*100:.2f}% R={recall[i]*100:.2f}% F1={f1[i]*100:.2f}%\")\n",
    "print()\n",
    "print(\"  Quality Checks:\")\n",
    "print(\"  \" + \"-\"*60)\n",
    "print(f\"  Overfitting (Train-Val gap):    {acc_gap*100:.2f}% {'âœ“' if acc_gap < 0.05 else 'âš '}\")\n",
    "print(f\"  Accuracy-F1 alignment:          {acc_f1_diff*100:.2f}% {'âœ“' if acc_f1_diff < 0.03 else 'âš '}\")\n",
    "print(f\"  Metric balance (healthy):       {'âœ“' if max(abs(precision[0]-recall[0]), abs(precision[0]-f1[0])) < 0.05 else 'âš '}\")\n",
    "print(f\"  Metric balance (unhealthy):     {'âœ“' if max(abs(precision[1]-recall[1]), abs(precision[1]-f1[1])) < 0.05 else 'âš '}\")\n",
    "print()\n",
    "print(\"  Files:\")\n",
    "print(\"  \" + \"-\"*60)\n",
    "print(f\"  Model:                   {MODEL_DIR}/best_model.keras\")\n",
    "print(f\"  Model Info:              {MODEL_DIR}/model_info.json\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"                         âœ“ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
