{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut Disease Detection Model v2\n",
    "## MobileNetV2 + Focal Loss for Leaf Disease Classification\n",
    "\n",
    "**Author:** Research Team  \n",
    "**Date:** January 2026  \n",
    "**Dataset:** Coconut Leaf Disease Dataset (4 classes)  \n",
    "\n",
    "### Objectives:\n",
    "- Train a deep learning model to classify coconut leaf diseases\n",
    "- Handle class imbalance using Focal Loss\n",
    "- Achieve balanced precision, recall, and F1-score across all classes\n",
    "- Prevent overfitting using proper regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Check Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "GPU Available: []\n",
      "NumPy Version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION\n",
      "======================================================================\n",
      "Image Size: 224x224\n",
      "Batch Size: 32\n",
      "Phase 1 Epochs: 10\n",
      "Phase 2 Epochs: 40\n",
      "Initial Learning Rate: 0.001\n",
      "Dropout Rate: 0.4\n",
      "Label Smoothing: 0.1\n",
      "Focal Loss Gamma: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = r'D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data', 'raw', 'stage_2_split')\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models', 'disease_detection_v2')\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "PHASE1_EPOCHS = 10  # Frozen base training\n",
    "PHASE2_EPOCHS = 40  # Fine-tuning\n",
    "INITIAL_LR = 0.001\n",
    "DROPOUT_RATE = 0.4\n",
    "LABEL_SMOOTHING = 0.1\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Phase 1 Epochs: {PHASE1_EPOCHS}\")\n",
    "print(f\"Phase 2 Epochs: {PHASE2_EPOCHS}\")\n",
    "print(f\"Initial Learning Rate: {INITIAL_LR}\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"Label Smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"Focal Loss Gamma: {FOCAL_GAMMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Focal Loss\n",
    "\n",
    "Focal Loss helps handle class imbalance by down-weighting easy examples and focusing on hard ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss configured with:\n",
      "  - Gamma: 2.0 (focus on hard examples)\n",
      "  - Alpha: 0.25 (class weight factor)\n",
      "  - Label Smoothing: 0.1\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss for handling class imbalance.\n",
    "    \n",
    "    Focal Loss adds a modulating factor (1-p)^gamma to cross-entropy loss,\n",
    "    focusing learning on hard misclassified examples.\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter. Higher values focus more on hard examples.\n",
    "        alpha: Weighting factor for the classes.\n",
    "        label_smoothing: Label smoothing factor for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, label_smoothing=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Apply label smoothing\n",
    "        if self.label_smoothing > 0:\n",
    "            num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "            y_true = y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        weight = self.alpha * y_true * tf.pow(1 - y_pred, self.gamma)\n",
    "        \n",
    "        # Apply focal weight to cross entropy\n",
    "        focal_loss = weight * cross_entropy\n",
    "        \n",
    "        return tf.reduce_sum(focal_loss, axis=-1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'gamma': self.gamma,\n",
    "            'alpha': self.alpha,\n",
    "            'label_smoothing': self.label_smoothing\n",
    "        })\n",
    "        return config\n",
    "\n",
    "print(\"Focal Loss configured with:\")\n",
    "print(f\"  - Gamma: {FOCAL_GAMMA} (focus on hard examples)\")\n",
    "print(f\"  - Alpha: {FOCAL_ALPHA} (class weight factor)\")\n",
    "print(f\"  - Label Smoothing: {LABEL_SMOOTHING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24998 images belonging to 4 classes.\n",
      "Found 838 images belonging to 4 classes.\n",
      "Found 837 images belonging to 4 classes.\n",
      "\n",
      "======================================================================\n",
      "DATASET SUMMARY\n",
      "======================================================================\n",
      "Classes: ['Leaf Rot', 'Leaf_Spot', 'healthy', 'not_cocount']\n",
      "Number of classes: 4\n",
      "Training samples: 24998\n",
      "Validation samples: 838\n",
      "Test samples: 837\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# No augmentation for validation/test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_DIR, 'train'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_DIR, 'val'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(DATA_DIR, 'test'),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get class information\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Test samples: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images visualization saved to: sample_images.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(DATA_DIR, 'train', class_name)\n",
    "    sample_images = os.listdir(class_dir)[:2]\n",
    "    \n",
    "    for j, img_name in enumerate(sample_images):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = plt.imread(img_path)\n",
    "        row = j\n",
    "        col = idx\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].set_title(class_name, fontsize=12)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'sample_images.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Sample images visualization saved to: sample_images.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASS WEIGHTS\n",
      "======================================================================\n",
      "Class weights (to handle imbalance):\n",
      "  Leaf Rot: 1.2499\n",
      "  Leaf_Spot: 1.2499\n",
      "  healthy: 1.2499\n",
      "  not_cocount: 0.6251\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLASS WEIGHTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Class weights (to handle imbalance):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {name}: {class_weights[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Model Architecture\n",
    "\n",
    "Using MobileNetV2 as the base model with a custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL ARCHITECTURE - MobileNetV2\n",
      "======================================================================\n",
      "Model: MobileNetV2 + Custom Head\n",
      "Total params: 2,625,476\n",
      "Trainable params: 363,012\n",
      "Non-trainable params: 2,262,464\n"
     ]
    }
   ],
   "source": [
    "def build_model(num_classes, dropout_rate=0.4):\n",
    "    \"\"\"Build MobileNetV2 model with custom classification head.\"\"\"\n",
    "    \n",
    "    # Load pre-trained MobileNetV2 (without top layers)\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build custom classification head\n",
    "    inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_model(num_classes, DROPOUT_RATE)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE - MobileNetV2\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: MobileNetV2 + Custom Head\")\n",
    "print(f\"Total params: {model.count_params():,}\")\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable = sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "print(f\"Trainable params: {trainable:,}\")\n",
    "print(f\"Non-trainable params: {non_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Phase 1: Train with Frozen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 1: TRAINING WITH FROZEN BASE\n",
      "======================================================================\n",
      "Training classifier head only (base frozen)...\n",
      "\n",
      "Epoch 1/10\n",
      "782/782 - 562s - accuracy: 0.8846 - loss: 0.0370 - val_accuracy: 0.9558 - val_loss: 0.0164 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "782/782 - 521s - accuracy: 0.9331 - loss: 0.0190 - val_accuracy: 0.9463 - val_loss: 0.0159 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "782/782 - 517s - accuracy: 0.9423 - loss: 0.0155 - val_accuracy: 0.9582 - val_loss: 0.0131 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "782/782 - 518s - accuracy: 0.9488 - loss: 0.0144 - val_accuracy: 0.9642 - val_loss: 0.0115 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "782/782 - 530s - accuracy: 0.9570 - loss: 0.0122 - val_accuracy: 0.9678 - val_loss: 0.0121 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "782/782 - 522s - accuracy: 0.9584 - loss: 0.0122 - val_accuracy: 0.9606 - val_loss: 0.0121 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "782/782 - 519s - accuracy: 0.9607 - loss: 0.0111 - val_accuracy: 0.9594 - val_loss: 0.0117 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "782/782 - 516s - accuracy: 0.9643 - loss: 0.0105 - val_accuracy: 0.9618 - val_loss: 0.0121 - lr: 0.0010\n",
      "\n",
      "Phase 1 Complete!\n",
      "Best Validation Accuracy: 96.78%\n"
     ]
    }
   ],
   "source": [
    "# Compile model for Phase 1\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=INITIAL_LR),\n",
    "    loss=FocalLoss(gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA, label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for Phase 1\n",
    "callbacks_phase1 = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 1: TRAINING WITH FROZEN BASE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Training classifier head only (base frozen)...\\n\")\n",
    "\n",
    "# Train Phase 1\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=PHASE1_EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_phase1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 1 Complete!\")\n",
    "print(f\"Best Validation Accuracy: {max(history_phase1.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phase 2: Fine-tuning\n",
    "\n",
    "Unfreeze top layers of base model for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 2: FINE-TUNING\n",
      "======================================================================\n",
      "Unfreezing top 50 layers for fine-tuning...\n",
      "Total layers: 156\n",
      "Trainable layers: 50\n",
      "\n",
      "Epoch 1/40\n",
      "782/782 - 598s - accuracy: 0.9758 - loss: 0.0073 - val_accuracy: 0.9857 - val_loss: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "782/782 - 595s - accuracy: 0.9832 - loss: 0.0054 - val_accuracy: 0.9845 - val_loss: 0.0052 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "782/782 - 596s - accuracy: 0.9864 - loss: 0.0046 - val_accuracy: 0.9869 - val_loss: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "782/782 - 598s - accuracy: 0.9888 - loss: 0.0040 - val_accuracy: 0.9881 - val_loss: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "782/782 - 601s - accuracy: 0.9897 - loss: 0.0036 - val_accuracy: 0.9905 - val_loss: 0.0040 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "782/782 - 599s - accuracy: 0.9905 - loss: 0.0034 - val_accuracy: 0.9869 - val_loss: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "782/782 - 602s - accuracy: 0.9917 - loss: 0.0030 - val_accuracy: 0.9869 - val_loss: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "782/782 - 604s - accuracy: 0.9921 - loss: 0.0029 - val_accuracy: 0.9869 - val_loss: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 5e-05\n",
      "Epoch 9/40\n",
      "782/782 - 605s - accuracy: 0.9929 - loss: 0.0027 - val_accuracy: 0.9893 - val_loss: 0.0042 - lr: 5.0000e-05\n",
      "Epoch 10/40\n",
      "782/782 - 607s - accuracy: 0.9934 - loss: 0.0027 - val_accuracy: 0.9881 - val_loss: 0.0049 - lr: 5.0000e-05\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\n",
      "Phase 2 Complete!\n",
      "Best Validation Accuracy: 99.05%\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze top layers for fine-tuning\n",
    "base_model.trainable = True\n",
    "fine_tune_at = len(base_model.layers) - 50  # Unfreeze last 50 layers\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=INITIAL_LR/10),\n",
    "    loss=FocalLoss(gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA, label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for Phase 2\n",
    "callbacks_phase2 = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Unfreezing top 50 layers for fine-tuning...\")\n",
    "print(f\"Total layers: {len(base_model.layers)}\")\n",
    "print(f\"Trainable layers: {len(base_model.layers) - fine_tune_at}\\n\")\n",
    "\n",
    "# Train Phase 2\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=PHASE2_EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_phase2,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 2 Complete!\")\n",
    "print(f\"Best Validation Accuracy: {max(history_phase2.history['val_accuracy'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EVALUATION ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "Test Accuracy: 98.69%\n",
      "Test Loss: 0.0039\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Classification Report (Class-wise Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASSIFICATION REPORT (Class-wise Metrics)\n",
      "======================================================================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Leaf Rot     1.0000    0.9760    0.9879       167\n",
      "   Leaf_Spot     0.9702    1.0000    0.9849       163\n",
      "     healthy     0.9605    0.9481    0.9542        77\n",
      " not_cocount     0.9930    0.9930    0.9930       430\n",
      "\n",
      "    accuracy                         0.9869       837\n",
      "   macro avg     0.9809    0.9793    0.9800       837\n",
      "weighted avg     0.9870    0.9869    0.9868       837\n",
      "\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASS-WISE METRICS\n",
      "======================================================================\n",
      "Class            Precision     Recall   F1-Score    Support\n",
      "---------------------------------------------------------------\n",
      "Leaf Rot            1.0000     0.9760     0.9879        167\n",
      "Leaf_Spot           0.9702     1.0000     0.9849        163\n",
      "healthy             0.9605     0.9481     0.9542         77\n",
      "not_cocount         0.9930     0.9930     0.9930        430\n",
      "---------------------------------------------------------------\n",
      "Macro Avg           0.9809     0.9793     0.9800\n",
      "\n",
      "======================================================================\n",
      "SUMMARY METRICS\n",
      "======================================================================\n",
      "Test Accuracy: 98.69%\n",
      "Macro F1-Score: 98.00%\n",
      "Difference (Acc - F1): 0.69%\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "test_generator.reset()\n",
    "predictions = model.predict(test_generator, verbose=0)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Classification Report\n",
    "print(\"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT (Class-wise Metrics)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Detailed metrics\n",
    "report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED CLASS-WISE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Class':<16} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n",
    "print(\"-\"*63)\n",
    "for class_name in class_names:\n",
    "    metrics = report_dict[class_name]\n",
    "    print(f\"{class_name:<16} {metrics['precision']:>10.4f} {metrics['recall']:>10.4f} {metrics['f1-score']:>10.4f} {int(metrics['support']):>10}\")\n",
    "print(\"-\"*63)\n",
    "print(f\"{'Macro Avg':<16} {report_dict['macro avg']['precision']:>10.4f} {report_dict['macro avg']['recall']:>10.4f} {report_dict['macro avg']['f1-score']:>10.4f}\")\n",
    "\n",
    "# Summary\n",
    "macro_f1 = report_dict['macro avg']['f1-score']\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Macro F1-Score: {macro_f1*100:.2f}%\")\n",
    "print(f\"Difference (Acc - F1): {abs(test_accuracy - macro_f1)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Disease Detection v2\\nTest Accuracy: {:.2f}%'.format(test_accuracy*100), \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Confusion matrix saved to: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history saved to: training_history.png\n"
     ]
    }
   ],
   "source": [
    "# Combine histories\n",
    "combined_history = {\n",
    "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
    "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
    "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
    "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss'],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(combined_history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(combined_history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].axvline(x=len(history_phase1.history['accuracy'])-1, color='r', linestyle='--', \n",
    "                label='Fine-tuning Start', alpha=0.7)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0.8, 1.0])\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(combined_history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(combined_history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].axvline(x=len(history_phase1.history['loss'])-1, color='r', linestyle='--', \n",
    "                label='Fine-tuning Start', alpha=0.7)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training History - Disease Detection v2', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Training history saved to: training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL INFORMATION SAVED\n",
      "======================================================================\n",
      "Model info saved to: model_info.json\n"
     ]
    }
   ],
   "source": [
    "# Save model info\n",
    "model_info = {\n",
    "    'model_name': 'disease_detection_v2',\n",
    "    'base_model': 'MobileNetV2',\n",
    "    'version': '2.0',\n",
    "    'date_trained': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'input_size': [IMG_SIZE, IMG_SIZE, 3],\n",
    "    'num_classes': num_classes,\n",
    "    'classes': class_names,\n",
    "    'hyperparameters': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'phase1_epochs': PHASE1_EPOCHS,\n",
    "        'phase2_epochs': PHASE2_EPOCHS,\n",
    "        'initial_lr': INITIAL_LR,\n",
    "        'dropout_rate': DROPOUT_RATE,\n",
    "        'label_smoothing': LABEL_SMOOTHING,\n",
    "        'focal_gamma': FOCAL_GAMMA,\n",
    "        'focal_alpha': FOCAL_ALPHA\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_samples': train_generator.samples,\n",
    "        'val_samples': val_generator.samples,\n",
    "        'test_samples': test_generator.samples\n",
    "    },\n",
    "    'metrics': {\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_loss': float(test_loss),\n",
    "        'macro_precision': float(report_dict['macro avg']['precision']),\n",
    "        'macro_recall': float(report_dict['macro avg']['recall']),\n",
    "        'macro_f1': float(report_dict['macro avg']['f1-score'])\n",
    "    },\n",
    "    'class_metrics': {\n",
    "        name: {\n",
    "            'precision': float(report_dict[name]['precision']),\n",
    "            'recall': float(report_dict[name]['recall']),\n",
    "            'f1_score': float(report_dict[name]['f1-score']),\n",
    "            'support': int(report_dict[name]['support'])\n",
    "        }\n",
    "        for name in class_names\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL INFORMATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(\"Model info saved to: model_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    TRAINING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Model saved to: D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\models\\disease_detection_v2\n",
      "\n",
      "======================================================================\n",
      "                    FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "  Test Accuracy:    98.69%\n",
      "  Macro F1-Score:   98.00%\n",
      "  Macro Precision:  98.09%\n",
      "  Macro Recall:     97.93%\n",
      "\n",
      "======================================================================\n",
      "                 CLASS-WISE F1-SCORES\n",
      "======================================================================\n",
      "\n",
      "  Leaf Rot:         98.79%\n",
      "  Leaf_Spot:        98.49%\n",
      "  healthy:          95.42%\n",
      "  not_cocount:      99.30%\n",
      "\n",
      "======================================================================\n",
      "                    FILES GENERATED\n",
      "======================================================================\n",
      "\n",
      "  - best_model.keras\n",
      "  - model_info.json\n",
      "  - confusion_matrix.png\n",
      "  - training_history.png\n",
      "  - sample_images.png\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"                    TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel saved to: {MODEL_DIR}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"                    FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n  Test Accuracy:    {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Macro F1-Score:   {macro_f1*100:.2f}%\")\n",
    "print(f\"  Macro Precision:  {report_dict['macro avg']['precision']*100:.2f}%\")\n",
    "print(f\"  Macro Recall:     {report_dict['macro avg']['recall']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"                 CLASS-WISE F1-SCORES\")\n",
    "print(\"=\"*70)\n",
    "for name in class_names:\n",
    "    print(f\"\\n  {name}:\" + \" \"*(15-len(name)) + f\"{report_dict[name]['f1-score']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"                    FILES GENERATED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n  - best_model.keras\")\n",
    "print(\"  - model_info.json\")\n",
    "print(\"  - confusion_matrix.png\")\n",
    "print(\"  - training_history.png\")\n",
    "print(\"  - sample_images.png\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
