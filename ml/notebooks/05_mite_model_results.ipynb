{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coconut Mite Detection Model - Training Results\n",
    "\n",
    "**Model:** MobileNetV2 (Transfer Learning)  \n",
    "**Dataset:** Pre-organized with NO data leakage  \n",
    "**Version:** v6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "MODEL_DIR = Path(r\"D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\models\\coconut_mite_v6\")\n",
    "DATA_DIR = Path(r\"D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\\data\\raw\\pest\")\n",
    "\n",
    "# Settings\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "CLASS_NAMES = ['coconut_mite', 'healthy']\n",
    "\n",
    "print(f\"Model Directory: {MODEL_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each split\n",
    "def count_images(data_dir, class_names):\n",
    "    counts = {}\n",
    "    splits_map = {\n",
    "        'Train': 'train',\n",
    "        'Validation': 'validation', \n",
    "        'Test': 'test'\n",
    "    }\n",
    "    \n",
    "    for split_name, split_key in splits_map.items():\n",
    "        counts[split_key] = {}\n",
    "        for cls in class_names:\n",
    "            # Try different folder names\n",
    "            for folder in [split_name, split_name.lower(), 'test', 'Test']:\n",
    "                cls_dir = data_dir / cls / folder\n",
    "                if cls_dir.exists():\n",
    "                    count = len([f for f in cls_dir.glob('*.*') \n",
    "                                if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])\n",
    "                    counts[split_key][cls] = count\n",
    "                    break\n",
    "            if cls not in counts[split_key]:\n",
    "                counts[split_key][cls] = 0\n",
    "    return counts\n",
    "\n",
    "counts = count_images(DATA_DIR, CLASS_NAMES)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET DISTRIBUTION (NO Data Leakage)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Split':<15} {'Mite':<10} {'Healthy':<10} {'Total':<10}\")\n",
    "print(\"-\"*45)\n",
    "total_all = 0\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    mite = counts[split].get('coconut_mite', 0)\n",
    "    healthy = counts[split].get('healthy', 0)\n",
    "    total = mite + healthy\n",
    "    total_all += total\n",
    "    note = \"(augmented)\" if split == 'train' else \"(originals)\"\n",
    "    print(f\"{split.capitalize():<15} {mite:<10} {healthy:<10} {total:<10} {note}\")\n",
    "print(\"-\"*45)\n",
    "print(f\"{'TOTAL':<15} {'':<10} {'':<10} {total_all:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "colors = ['#e74c3c', '#2ecc71']  # Red for mite, green for healthy\n",
    "\n",
    "for ax, split in zip(axes, ['train', 'validation', 'test']):\n",
    "    vals = [counts[split].get(c, 0) for c in CLASS_NAMES]\n",
    "    total = sum(vals)\n",
    "    bars = ax.bar(CLASS_NAMES, vals, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    title_suffix = \"(augmented)\" if split == 'train' else \"(originals)\"\n",
    "    ax.set_title(f'{split.capitalize()} Set\\n{total} images {title_suffix}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Images')\n",
    "    ax.set_ylim([0, max(vals) * 1.2 if max(vals) > 0 else 10])\n",
    "    \n",
    "    for bar, v in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, v + max(vals)*0.02, \n",
    "                f'{v}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.suptitle('Dataset Distribution (NO Data Leakage)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"  - Training set contains augmented images\")\n",
    "print(\"  - Validation and Test sets contain ONLY original images\")\n",
    "print(\"  - NO overlap between splits (no data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = MODEL_DIR / 'best_model.keras'\n",
    "model = tf.keras.models.load_model(str(model_path))\n",
    "\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"  Total Parameters: {model.count_params():,}\")\n",
    "trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"  Trainable Parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "history_path = MODEL_DIR / 'training_history.json'\n",
    "with open(history_path, 'r') as f:\n",
    "    history = json.load(f)\n",
    "\n",
    "epochs_trained = len(history['accuracy'])\n",
    "best_val_acc = max(history['val_accuracy'])\n",
    "best_epoch = history['val_accuracy'].index(best_val_acc) + 1\n",
    "\n",
    "print(f\"Training History Loaded\")\n",
    "print(f\"  Epochs Trained: {epochs_trained}\")\n",
    "print(f\"  Best Validation Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch})\")\n",
    "print(f\"  Final Training Accuracy: {history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Final Validation Accuracy: {history['val_accuracy'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "epochs_range = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, history['accuracy'], 'b-o', label='Training', markersize=3, linewidth=1.5)\n",
    "axes[0].plot(epochs_range, history['val_accuracy'], 'r-s', label='Validation', markersize=3, linewidth=1.5)\n",
    "axes[0].scatter([best_epoch], [best_val_acc], color='green', s=200, zorder=5, marker='*',\n",
    "                label=f'Best: {best_val_acc*100:.2f}%')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history['loss'], 'b-o', label='Training', markersize=3, linewidth=1.5)\n",
    "axes[1].plot(epochs_range, history['val_loss'], 'r-s', label='Validation', markersize=3, linewidth=1.5)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate gap\n",
    "final_train_acc = history['accuracy'][-1]\n",
    "final_val_acc = history['val_accuracy'][-1]\n",
    "gap = abs(final_train_acc - final_val_acc)\n",
    "\n",
    "plt.suptitle(f'Training History (Train-Val Gap: {gap*100:.2f}%)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Analysis:\")\n",
    "print(f\"  Final Training Accuracy: {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
    "print(f\"  Train-Val Gap: {gap*100:.2f}%\")\n",
    "if gap > 0.15:\n",
    "    print(\"  Status: OVERFITTING detected!\")\n",
    "elif gap < 0.05:\n",
    "    print(\"  Status: Good generalization\")\n",
    "else:\n",
    "    print(\"  Status: Acceptable generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset from TF structure\n",
    "TF_DATA_DIR = MODEL_DIR / 'tf_data'\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(TF_DATA_DIR / 'test'),\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "detected_classes = test_ds.class_names\n",
    "print(f\"Test dataset loaded\")\n",
    "print(f\"Detected classes: {detected_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample test images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Sample Test Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i].numpy().astype('uint8'))\n",
    "            cls_name = detected_classes[labels[i]]\n",
    "            color = 'red' if 'mite' in cls_name else 'green'\n",
    "            ax.set_title(cls_name, fontsize=10, color=color)\n",
    "            ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for evaluation\n",
    "normalization = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = normalization(image)\n",
    "    return image, label\n",
    "\n",
    "test_ds_prep = test_ds.map(preprocess).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Get predictions\n",
    "print(\"Running predictions on test set...\")\n",
    "y_true = []\n",
    "y_pred_probs = []\n",
    "\n",
    "for images, labels in test_ds_prep:\n",
    "    preds = model.predict(images, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred_probs.extend(preds.flatten())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred_probs = np.array(y_pred_probs)\n",
    "\n",
    "print(f\"Test samples: {len(y_true)}\")\n",
    "print(f\"Class distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model info for optimal threshold\n",
    "info_path = MODEL_DIR / 'model_info.json'\n",
    "with open(info_path, 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "OPTIMAL_THRESHOLD = model_info.get('threshold', 0.45)\n",
    "print(f\"Using optimal threshold: {OPTIMAL_THRESHOLD}\")\n",
    "\n",
    "# Apply threshold\n",
    "y_pred = (y_pred_probs > OPTIMAL_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=detected_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=detected_classes, yticklabels=detected_classes, annot_kws={'size': 20})\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Normalized\n",
    "cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=detected_classes, yticklabels=detected_classes, annot_kws={'size': 16})\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "p, r, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "# Performance visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "metrics_values = [accuracy, np.mean(p), np.mean(r), macro_f1]\n",
    "colors_metrics = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "bars = axes[0].bar(metrics_names, metrics_values, color=colors_metrics, edgecolor='black')\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].set_title('Overall Model Performance', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "for bar, val in zip(bars, metrics_values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.02, f'{val:.2%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Per-class metrics\n",
    "x = np.arange(len(detected_classes))\n",
    "width = 0.25\n",
    "axes[1].bar(x - width, p, width, label='Precision', color='#3498db', edgecolor='black')\n",
    "axes[1].bar(x, r, width, label='Recall', color='#2ecc71', edgecolor='black')\n",
    "axes[1].bar(x + width, f1, width, label='F1-Score', color='#e74c3c', edgecolor='black')\n",
    "axes[1].set_ylim([0, 1.1])\n",
    "axes[1].set_title('Per-Class Performance', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(detected_classes)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_DIR / 'performance_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Madam's Requirements Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MADAM'S REQUIREMENTS VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_pass = True\n",
    "TOLERANCE = 0.10  # 10% tolerance\n",
    "\n",
    "# Requirement 1: P/R/F1 balanced per class\n",
    "print(\"\\n[1] P/R/F1 should be close for EACH class (gap < 10%)\")\n",
    "print(\"-\"*60)\n",
    "for i, cls in enumerate(detected_classes):\n",
    "    pr_gap = abs(p[i] - r[i])\n",
    "    status = \"PASS\" if pr_gap < TOLERANCE else \"FAIL\"\n",
    "    if pr_gap >= TOLERANCE:\n",
    "        all_pass = False\n",
    "    print(f\"  {cls}:\")\n",
    "    print(f\"    Precision: {p[i]*100:.2f}%\")\n",
    "    print(f\"    Recall:    {r[i]*100:.2f}%\")\n",
    "    print(f\"    F1-Score:  {f1[i]*100:.2f}%\")\n",
    "    print(f\"    P-R Gap:   {pr_gap*100:.2f}% [{status}]\")\n",
    "\n",
    "# Requirement 2: F1 similar across classes\n",
    "print(\"\\n[2] F1-Scores should be similar across classes (diff < 10%)\")\n",
    "print(\"-\"*60)\n",
    "f1_diff = abs(f1[0] - f1[1]) if len(f1) > 1 else 0\n",
    "status = \"PASS\" if f1_diff < TOLERANCE else \"FAIL\"\n",
    "if f1_diff >= TOLERANCE:\n",
    "    all_pass = False\n",
    "for i, cls in enumerate(detected_classes):\n",
    "    print(f\"  {cls} F1: {f1[i]*100:.2f}%\")\n",
    "print(f\"  F1 Difference: {f1_diff*100:.2f}% [{status}]\")\n",
    "\n",
    "# Requirement 3: Accuracy close to F1\n",
    "print(\"\\n[3] Accuracy should be close to F1-Score (diff < 10%)\")\n",
    "print(\"-\"*60)\n",
    "acc_f1_diff = abs(accuracy - macro_f1)\n",
    "status = \"PASS\" if acc_f1_diff < TOLERANCE else \"FAIL\"\n",
    "if acc_f1_diff >= TOLERANCE:\n",
    "    all_pass = False\n",
    "print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"  Macro F1:  {macro_f1*100:.2f}%\")\n",
    "print(f\"  Difference: {acc_f1_diff*100:.2f}% [{status}]\")\n",
    "\n",
    "# Requirement 4: No overfitting\n",
    "print(\"\\n[4] Train-Val gap should be small (no overfitting, gap < 15%)\")\n",
    "print(\"-\"*60)\n",
    "train_val_gap = abs(final_train_acc - final_val_acc)\n",
    "status = \"PASS\" if train_val_gap < 0.15 else \"FAIL\"\n",
    "if train_val_gap >= 0.15:\n",
    "    all_pass = False\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {final_val_acc*100:.2f}%\")\n",
    "print(f\"  Gap: {train_val_gap*100:.2f}% [{status}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_pass:\n",
    "    print(\"ALL REQUIREMENTS PASSED!\")\n",
    "else:\n",
    "    print(\"SOME REQUIREMENTS NEED ATTENTION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n--- Model ---\")\n",
    "print(f\"  Architecture: MobileNetV2 (Transfer Learning)\")\n",
    "print(f\"  Input Size: {IMG_SIZE[0]}x{IMG_SIZE[1]}x3\")\n",
    "print(f\"  Threshold: {OPTIMAL_THRESHOLD}\")\n",
    "\n",
    "print(f\"\\n--- Dataset (NO DATA LEAKAGE) ---\")\n",
    "print(f\"  Training: {sum(counts['train'].values())} images (with augmentations)\")\n",
    "print(f\"  Validation: {sum(counts['validation'].values())} images (originals only)\")\n",
    "print(f\"  Test: {sum(counts['test'].values())} images (originals only)\")\n",
    "\n",
    "print(f\"\\n--- Training ---\")\n",
    "print(f\"  Epochs: {epochs_trained}\")\n",
    "print(f\"  Best Epoch: {best_epoch}\")\n",
    "print(f\"  Best Val Accuracy: {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Train-Val Gap: {train_val_gap*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n--- Test Results ---\")\n",
    "print(f\"  Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"  Macro F1-Score: {macro_f1*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n--- Per-Class Results ---\")\n",
    "for i, cls in enumerate(detected_classes):\n",
    "    print(f\"  {cls}:\")\n",
    "    print(f\"    Precision: {p[i]*100:.2f}%\")\n",
    "    print(f\"    Recall: {r[i]*100:.2f}%\")\n",
    "    print(f\"    F1-Score: {f1[i]*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n--- Files ---\")\n",
    "print(f\"  Model: {MODEL_DIR / 'best_model.keras'}\")\n",
    "print(f\"  Info: {MODEL_DIR / 'model_info.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_pass:\n",
    "    print(\"ALL MADAM'S REQUIREMENTS: PASSED\")\n",
    "else:\n",
    "    print(\"SOME REQUIREMENTS NEED ATTENTION\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
