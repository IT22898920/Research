{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Coconut Leaf Disease Detection Model v1\n",
    "\n",
    "## Coconut Health Monitor - Research Project\n",
    "\n",
    "**Objective:** Train a model to detect leaf diseases in coconut trees.\n",
    "\n",
    "### Model Details\n",
    "- **Architecture:** EfficientNetB0 (Transfer Learning)\n",
    "- **Classes:** 3 (healthy, Leaf Rot, Leaf_Spot)\n",
    "- **Input Size:** 224x224x3\n",
    "- **Loss Function:** Focal Loss (for class imbalance)\n",
    "\n",
    "### Disease Detection Categories\n",
    "1. **Healthy** - Normal healthy coconut leaves\n",
    "2. **Leaf Rot** - Leaves affected by rot disease\n",
    "3. **Leaf Spot** - Leaves with spot disease\n",
    "\n",
    "### Anti-Overfitting Measures\n",
    "1. Data augmentation (training only)\n",
    "2. Dropout layers\n",
    "3. Early stopping\n",
    "4. Learning rate reduction\n",
    "5. Class weights for imbalanced data\n",
    "6. 2-phase training (frozen base â†’ fine-tuning)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"leaf_disease_detection_v1\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = ['Leaf Rot', 'Leaf_Spot', 'healthy']  # Alphabetical order\n",
    "\n",
    "# Training Configuration\n",
    "PHASE1_EPOCHS = 25  # Frozen base layers\n",
    "PHASE2_EPOCHS = 35  # Fine-tuning\n",
    "PHASE1_LR = 1e-3\n",
    "PHASE2_LR = 1e-5\n",
    "\n",
    "# Focal Loss Parameters\n",
    "FOCAL_GAMMA = 2.0\n",
    "FOCAL_ALPHA = 0.25\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = Path(r\"D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml\")\n",
    "DATA_PATH = BASE_PATH / \"data\" / \"raw\" / \"stage_2_split\"\n",
    "MODEL_SAVE_PATH = BASE_PATH / \"models\" / MODEL_NAME\n",
    "\n",
    "# Create output directory\n",
    "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model Name: {MODEL_NAME}\")\n",
    "print(f\"  Image Size: {IMG_SIZE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Classes: {CLASS_NAMES}\")\n",
    "print(f\"  Phase 1 Epochs: {PHASE1_EPOCHS}\")\n",
    "print(f\"  Phase 2 Epochs: {PHASE2_EPOCHS}\")\n",
    "print(f\"\\nData Path: {DATA_PATH}\")\n",
    "print(f\"Model Save Path: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_analysis",
   "metadata": {},
   "source": [
    "## 3. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(data_path):\n",
    "    \"\"\"Analyze the dataset and display statistics.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Dataset Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = data_path / split\n",
    "        stats[split] = {}\n",
    "        \n",
    "        for class_dir in split_path.iterdir():\n",
    "            if class_dir.is_dir():\n",
    "                class_name = class_dir.name\n",
    "                n_images = len(list(class_dir.glob('*')))\n",
    "                stats[split][class_name] = n_images\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(stats).T\n",
    "    df['Total'] = df.sum(axis=1)\n",
    "    \n",
    "    print(\"\\nDataset Distribution:\")\n",
    "    print(df)\n",
    "    \n",
    "    total_images = df['Total'].sum()\n",
    "    print(f\"\\nTotal Images: {total_images}\")\n",
    "    print(f\"  Train: {df.loc['train', 'Total']} ({df.loc['train', 'Total']/total_images*100:.1f}%)\")\n",
    "    print(f\"  Validation: {df.loc['val', 'Total']} ({df.loc['val', 'Total']/total_images*100:.1f}%)\")\n",
    "    print(f\"  Test: {df.loc['test', 'Total']} ({df.loc['test', 'Total']/total_images*100:.1f}%)\")\n",
    "    \n",
    "    return stats, df\n",
    "\n",
    "stats, stats_df = analyze_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset_distribution(stats, save_path):\n",
    "    \"\"\"Visualize dataset distribution.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.suptitle('Leaf Disease Dataset Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\n",
    "    \n",
    "    for idx, split in enumerate(['train', 'val', 'test']):\n",
    "        ax = axes[idx]\n",
    "        data = stats[split]\n",
    "        \n",
    "        classes = list(data.keys())\n",
    "        counts = list(data.values())\n",
    "        \n",
    "        ax.bar(classes, counts, color=colors)\n",
    "        ax.set_title(f'{split.upper()} Set ({sum(counts)} images)', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Class')\n",
    "        ax.set_ylabel('Number of Images')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add count labels on bars\n",
    "        for i, (c, count) in enumerate(zip(classes, counts)):\n",
    "            ax.text(i, count + max(counts)*0.02, str(count), \n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'dataset_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset_distribution(stats, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(data_path, save_path):\n",
    "    \"\"\"Display sample images from each class.\"\"\"\n",
    "    \n",
    "    train_path = data_path / 'train'\n",
    "    class_dirs = sorted([d for d in train_path.iterdir() if d.is_dir()])\n",
    "    \n",
    "    fig, axes = plt.subplots(len(class_dirs), 4, figsize=(14, 10))\n",
    "    fig.suptitle('Sample Images from Each Class', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for row, cls_dir in enumerate(class_dirs):\n",
    "        images = list(cls_dir.glob('*'))[:4]\n",
    "        \n",
    "        for col in range(4):\n",
    "            ax = axes[row, col]\n",
    "            if col < len(images):\n",
    "                img = Image.open(images[col])\n",
    "                ax.imshow(img)\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(cls_dir.name, fontsize=11, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'sample_images.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "display_sample_images(DATA_PATH, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_generators",
   "metadata": {},
   "source": [
    "## 4. Data Generators with Augmentation\n",
    "\n",
    "**IMPORTANT:** Data augmentation is applied ONLY to training data to prevent data leaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_generators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Generator WITH Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation & Test Data Generator WITHOUT Augmentation (only rescaling)\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "print(\"Data Generators Created:\")\n",
    "print(\"  Training: WITH augmentation (rotation, shift, flip, zoom)\")\n",
    "print(\"  Validation/Test: WITHOUT augmentation (only rescaling)\")\n",
    "print(\"\\n  This prevents data leaking from augmented images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flow_generators",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATA_PATH / 'train',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    DATA_PATH / 'val',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    DATA_PATH / 'test',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Verify class indices\n",
    "print(\"\\nClass Indices:\")\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "# Create reverse mapping\n",
    "INDEX_TO_CLASS = {v: k for k, v in train_generator.class_indices.items()}\n",
    "print(\"\\nIndex to Class Mapping:\")\n",
    "print(INDEX_TO_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class_weights",
   "metadata": {},
   "source": [
    "## 5. Compute Class Weights for Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_weights",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(generator):\n",
    "    \"\"\"Compute class weights to handle class imbalance.\"\"\"\n",
    "    \n",
    "    # Get all labels\n",
    "    labels = generator.classes\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(labels),\n",
    "        y=labels\n",
    "    )\n",
    "    \n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(\"Class Weights (to handle imbalance):\")\n",
    "    for idx, weight in class_weight_dict.items():\n",
    "        class_name = INDEX_TO_CLASS[idx]\n",
    "        count = np.sum(labels == idx)\n",
    "        print(f\"  {class_name}: {weight:.4f} (n={count})\")\n",
    "    \n",
    "    return class_weight_dict\n",
    "\n",
    "class_weights = compute_class_weights(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal_loss",
   "metadata": {},
   "source": [
    "## 6. Define Focal Loss Function\n",
    "\n",
    "Focal Loss helps with class imbalance by down-weighting easy examples and focusing on hard examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_focal_loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification.\n",
    "    \n",
    "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Args:\n",
    "        gamma: Focusing parameter (default 2.0)\n",
    "        alpha: Weighting factor (default 0.25)\n",
    "    \"\"\"\n",
    "    def focal_loss_fn(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * tf.keras.backend.log(y_pred)\n",
    "        focal_weight = tf.keras.backend.pow(1.0 - y_pred, gamma)\n",
    "        focal_loss = alpha * focal_weight * cross_entropy\n",
    "        \n",
    "        return tf.keras.backend.sum(focal_loss, axis=-1)\n",
    "    \n",
    "    return focal_loss_fn\n",
    "\n",
    "print(f\"Focal Loss configured with gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA}\")\n",
    "print(\"\\nFocal Loss helps the model focus on hard-to-classify examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_model",
   "metadata": {},
   "source": [
    "## 7. Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, trainable_base=False):\n",
    "    \"\"\"\n",
    "    Build EfficientNetB0-based model for leaf disease classification.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        trainable_base: Whether base model is trainable\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "        base_model: Base EfficientNetB0 model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-trained EfficientNetB0\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = trainable_base\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model with frozen base\n",
    "model, base_model = build_model(NUM_CLASSES, trainable_base=False)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "count_params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Non-trainable: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compile_model",
   "metadata": {},
   "source": [
    "## 8. Compile Model and Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compile_phase1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for Phase 1 (frozen base)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=PHASE1_LR),\n",
    "    loss=focal_loss(gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled for Phase 1 (frozen base):\")\n",
    "print(f\"  Optimizer: Adam (lr={PHASE1_LR})\")\n",
    "print(f\"  Loss: Focal Loss (gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_callbacks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=8,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=str(MODEL_SAVE_PATH / 'best_model_phase1.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  1. EarlyStopping (patience=8)\")\n",
    "print(\"  2. ReduceLROnPlateau (factor=0.5, patience=4)\")\n",
    "print(\"  3. ModelCheckpoint (save best model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_training",
   "metadata": {},
   "source": [
    "## 9. Phase 1 Training - Frozen Base Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_phase1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: Training with Frozen Base Layers\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining for up to {PHASE1_EPOCHS} epochs...\")\n",
    "print(f\"Base model (EfficientNetB0) is FROZEN - only training top layers.\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=PHASE1_EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase1_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nPhase 1 completed in {phase1_time:.2f} minutes\")\n",
    "print(f\"Best validation accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_training",
   "metadata": {},
   "source": [
    "## 10. Phase 2 Training - Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unfreeze_base",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 2: Fine-tuning with Unfrozen Base Layers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers, unfreeze later layers\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_layers = sum([1 for layer in base_model.layers if layer.trainable])\n",
    "print(f\"\\nUnfrozen {trainable_layers} layers in base model for fine-tuning.\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=PHASE2_LR),\n",
    "    loss=focal_loss(gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Recompiled with learning rate: {PHASE2_LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_phase2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update checkpoint path for phase 2\n",
    "callbacks_list_phase2 = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=str(MODEL_SAVE_PATH / 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nTraining for up to {PHASE2_EPOCHS} epochs...\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=PHASE2_EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_list_phase2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "phase2_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nPhase 2 completed in {phase2_time:.2f} minutes\")\n",
    "print(f\"Best validation accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_history",
   "metadata": {},
   "source": [
    "## 11. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_history",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history1, history2, save_path):\n",
    "    \"\"\"Plot combined training history for both phases.\"\"\"\n",
    "    \n",
    "    # Combine histories\n",
    "    acc = history1.history['accuracy'] + history2.history['accuracy']\n",
    "    val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
    "    loss = history1.history['loss'] + history2.history['loss']\n",
    "    val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    phase1_end = len(history1.history['accuracy'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Training History - Leaf Disease Detection Model', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(epochs, acc, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax1.plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax1.axvline(x=phase1_end, color='g', linestyle='--', label='Phase 1 â†’ Phase 2')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(epochs, loss, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax2.plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax2.axvline(x=phase1_end, color='g', linestyle='--', label='Phase 1 â†’ Phase 2')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history_phase1, history_phase2, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Model Evaluation on Test Set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(\n",
    "    MODEL_SAVE_PATH / 'best_model.keras',\n",
    "    custom_objects={'focal_loss_fn': focal_loss(gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA)}\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = best_model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classification_report",
   "metadata": {},
   "source": [
    "## 13. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "get_predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "test_generator.reset()\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report (Class-wise Metrics)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get class names in correct order\n",
    "class_names_ordered = [k for k, v in sorted(test_generator.class_indices.items(), key=lambda x: x[1])]\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names_ordered, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names_ordered,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Class-wise Metrics Summary\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate macro averages\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Overall Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Accuracy:        {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Macro Recall:    {macro_recall:.4f}\")\n",
    "print(f\"  Macro F1-Score:  {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_matrix",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path):\n",
    "    \"\"\"Plot confusion matrix with percentages.\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Confusion Matrix - Leaf Disease Detection Model', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_title('Counts')\n",
    "    \n",
    "    # Percentages\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=axes[1])\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].set_title('Percentages (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, class_names_ordered, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "per_class_metrics",
   "metadata": {},
   "source": [
    "## 15. Per-Class Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class_metrics(metrics_df, save_path):\n",
    "    \"\"\"Visualize precision, recall, and F1-score per class.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(metrics_df))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax.bar(x - width, metrics_df['Precision'], width, label='Precision', color='#3498db')\n",
    "    bars2 = ax.bar(x, metrics_df['Recall'], width, label='Recall', color='#2ecc71')\n",
    "    bars3 = ax.bar(x + width, metrics_df['F1-Score'], width, label='F1-Score', color='#e74c3c')\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Per-Class Metrics: Precision, Recall, F1-Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_df['Class'])\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_per_class_metrics(metrics_df, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_predictions",
   "metadata": {},
   "source": [
    "## 16. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(generator, model, save_path, n_samples=12):\n",
    "    \"\"\"Display sample predictions with confidence scores.\"\"\"\n",
    "    \n",
    "    generator.reset()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, labels = next(generator)\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    \n",
    "    # Select samples\n",
    "    n_samples = min(n_samples, len(images))\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    fig.suptitle('Sample Predictions with Confidence Scores', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    class_names = list(generator.class_indices.keys())\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        ax = axes[idx // 4, idx % 4]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(images[idx])\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_idx = np.argmax(predictions[idx])\n",
    "        pred_class = class_names[pred_idx]\n",
    "        pred_conf = predictions[idx][pred_idx]\n",
    "        \n",
    "        true_idx = np.argmax(labels[idx])\n",
    "        true_class = class_names[true_idx]\n",
    "        \n",
    "        # Set title color based on correctness\n",
    "        color = 'green' if pred_idx == true_idx else 'red'\n",
    "        \n",
    "        ax.set_title(f'True: {true_class}\\nPred: {pred_class} ({pred_conf:.2%})', \n",
    "                    fontsize=10, color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path / 'sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_predictions(test_generator, best_model, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_info",
   "metadata": {},
   "source": [
    "## 17. Save Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total training time\n",
    "total_time = phase1_time + phase2_time\n",
    "total_epochs = len(history_phase1.history['accuracy']) + len(history_phase2.history['accuracy'])\n",
    "\n",
    "# Model information\n",
    "model_info = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'version': '1.0',\n",
    "    'architecture': 'EfficientNetB0',\n",
    "    'framework': 'TensorFlow/Keras',\n",
    "    'tensorflow_version': tf.__version__,\n",
    "    'image_size': IMG_SIZE,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': class_names_ordered,\n",
    "    'class_indices': test_generator.class_indices,\n",
    "    'training': {\n",
    "        'total_epochs': total_epochs,\n",
    "        'phase1_epochs': len(history_phase1.history['accuracy']),\n",
    "        'phase2_epochs': len(history_phase2.history['accuracy']),\n",
    "        'phase1_lr': PHASE1_LR,\n",
    "        'phase2_lr': PHASE2_LR,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'focal_gamma': FOCAL_GAMMA,\n",
    "        'focal_alpha': FOCAL_ALPHA,\n",
    "        'total_time_minutes': round(total_time, 2),\n",
    "        'optimizer': 'Adam',\n",
    "        'loss_function': 'Focal Loss'\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(test_accuracy),\n",
    "        'test_loss': float(test_loss),\n",
    "        'macro_precision': float(macro_precision),\n",
    "        'macro_recall': float(macro_recall),\n",
    "        'macro_f1': float(macro_f1)\n",
    "    },\n",
    "    'per_class_metrics': {\n",
    "        class_names_ordered[i]: {\n",
    "            'precision': float(precision[i]),\n",
    "            'recall': float(recall[i]),\n",
    "            'f1_score': float(f1[i]),\n",
    "            'support': int(support[i])\n",
    "        }\n",
    "        for i in range(len(class_names_ordered))\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_samples': train_generator.samples,\n",
    "        'val_samples': validation_generator.samples,\n",
    "        'test_samples': test_generator.samples,\n",
    "        'total_samples': train_generator.samples + validation_generator.samples + test_generator.samples\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'rotation_range': 30,\n",
    "        'width_shift_range': 0.2,\n",
    "        'height_shift_range': 0.2,\n",
    "        'shear_range': 0.2,\n",
    "        'zoom_range': 0.2,\n",
    "        'horizontal_flip': True,\n",
    "        'vertical_flip': True\n",
    "    },\n",
    "    'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save model info\n",
    "with open(MODEL_SAVE_PATH / 'model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"Model information saved to: {MODEL_SAVE_PATH / 'model_info.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_summary",
   "metadata": {},
   "source": [
    "## 18. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"           TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model: {MODEL_NAME}\")\n",
    "print(f\"   Architecture: EfficientNetB0 (Transfer Learning)\")\n",
    "print(f\"   Classes: {', '.join(class_names_ordered)}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  Training Time: {total_time:.2f} minutes ({total_epochs} epochs)\")\n",
    "print(f\"   Phase 1: {len(history_phase1.history['accuracy'])} epochs ({phase1_time:.2f} min)\")\n",
    "print(f\"   Phase 2: {len(history_phase2.history['accuracy'])} epochs ({phase2_time:.2f} min)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Test Results:\")\n",
    "print(f\"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {macro_precision:.4f}\")\n",
    "print(f\"   Recall:    {macro_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Per-Class Performance:\")\n",
    "for i, cls in enumerate(class_names_ordered):\n",
    "    print(f\"   {cls:15s} - P: {precision[i]:.4f}, R: {recall[i]:.4f}, F1: {f1[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Saved Files:\")\n",
    "print(f\"   - best_model.keras\")\n",
    "print(f\"   - best_model_phase1.keras\")\n",
    "print(f\"   - model_info.json\")\n",
    "print(f\"   - dataset_distribution.png\")\n",
    "print(f\"   - sample_images.png\")\n",
    "print(f\"   - training_history.png\")\n",
    "print(f\"   - confusion_matrix.png\")\n",
    "print(f\"   - per_class_metrics.png\")\n",
    "print(f\"   - sample_predictions.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"     Model ready for deployment in Flask API!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
