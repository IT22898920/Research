{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Caterpillar & White Fly Detection Model v2\n",
    "\n",
    "## Model Information\n",
    "- **Model Name**: unified_caterpillar_whitefly_v2\n",
    "- **Architecture**: EfficientNetB0 (Transfer Learning)\n",
    "- **Classes**: caterpillar, white_fly, healthy, not_coconut\n",
    "- **Input Size**: 224x224x3\n",
    "\n",
    "## Improvements over v1\n",
    "- EfficientNetB0 instead of MobileNetV2 (better accuracy)\n",
    "- No duplicate data (healthy/not_coconut from one source only)\n",
    "- Stronger data augmentation\n",
    "- Mixup augmentation\n",
    "- Label smoothing\n",
    "\n",
    "## Dataset Sources\n",
    "- caterpillar: `pest_caterpillar/dataset/`\n",
    "- white_fly: `white_fly/`\n",
    "- healthy: `pest_caterpillar/dataset/` ONLY\n",
    "- not_coconut: `pest_caterpillar/dataset/` ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'unified_caterpillar_whitefly_v2',\n",
    "    'img_size': (224, 224),\n",
    "    'batch_size': 32,\n",
    "    'classes': ['caterpillar', 'healthy', 'not_coconut', 'white_fly'],\n",
    "    'phase1_epochs': 20,\n",
    "    'phase2_epochs': 30,\n",
    "    'learning_rate_phase1': 1e-3,\n",
    "    'learning_rate_phase2': 1e-5,\n",
    "    'focal_gamma': 2.5,  # Increased for harder examples\n",
    "    'label_smoothing': 0.1,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'dropout_rate': 0.4,\n",
    "}\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = Path(r'D:\\SLIIT\\Reaserch Project\\CoconutHealthMonitor\\Research\\ml')\n",
    "DATA_PATH = BASE_PATH / 'data' / 'raw'\n",
    "MODEL_SAVE_PATH = BASE_PATH / 'models' / CONFIG['model_name']\n",
    "MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset paths - NO DUPLICATES\n",
    "CATERPILLAR_PATH = DATA_PATH / 'pest_caterpillar' / 'dataset'\n",
    "WHITEFLY_PATH = DATA_PATH / 'white_fly'\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nModel save path: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Focal Loss with Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossWithSmoothing(keras.losses.Loss):\n",
    "    \"\"\"Focal Loss with Label Smoothing for class imbalance.\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=None, label_smoothing=0.0, num_classes=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Apply label smoothing\n",
    "        if self.label_smoothing > 0:\n",
    "            y_true = y_true * (1 - self.label_smoothing) + self.label_smoothing / self.num_classes\n",
    "        \n",
    "        # Clip predictions\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = y_true * tf.pow(1 - y_pred, self.gamma)\n",
    "        focal_loss = weight * ce\n",
    "        \n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'gamma': self.gamma,\n",
    "            'alpha': self.alpha,\n",
    "            'label_smoothing': self.label_smoothing,\n",
    "            'num_classes': self.num_classes\n",
    "        })\n",
    "        return config\n",
    "\n",
    "print(\"Focal Loss with Label Smoothing defined.\")\n",
    "print(f\"  Gamma: {CONFIG['focal_gamma']}\")\n",
    "print(f\"  Label Smoothing: {CONFIG['label_smoothing']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Unified Dataset (No Duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_dataset_v2():\n",
    "    \"\"\"\n",
    "    Create unified dataset WITHOUT duplicate healthy/not_coconut data.\n",
    "    - caterpillar: from pest_caterpillar/dataset\n",
    "    - white_fly: from white_fly\n",
    "    - healthy: from pest_caterpillar/dataset ONLY\n",
    "    - not_coconut: from pest_caterpillar/dataset ONLY\n",
    "    \"\"\"\n",
    "    unified_path = DATA_PATH / 'unified_caterpillar_whitefly_v2'\n",
    "    \n",
    "    # Remove old unified dataset if exists\n",
    "    if unified_path.exists():\n",
    "        shutil.rmtree(unified_path)\n",
    "    \n",
    "    splits = ['train', 'validation', 'test']\n",
    "    \n",
    "    # Mapping for folder names\n",
    "    caterpillar_split_map = {'train': 'train', 'validation': 'validation', 'test': 'test'}\n",
    "    whitefly_split_map = {'train': 'Training', 'validation': 'validation', 'test': 'test'}\n",
    "    \n",
    "    stats = {split: {} for split in splits}\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"\\nProcessing {split} split...\")\n",
    "        \n",
    "        # Create directories for all classes\n",
    "        for cls in CONFIG['classes']:\n",
    "            (unified_path / split / cls).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 1. Copy caterpillar images\n",
    "        cat_split = caterpillar_split_map[split]\n",
    "        cat_src = CATERPILLAR_PATH / cat_split / 'caterpillar'\n",
    "        if cat_src.exists():\n",
    "            count = 0\n",
    "            for img in cat_src.glob('*'):\n",
    "                if img.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                    shutil.copy2(img, unified_path / split / 'caterpillar' / img.name)\n",
    "                    count += 1\n",
    "            stats[split]['caterpillar'] = count\n",
    "            print(f\"  caterpillar: {count} images\")\n",
    "        \n",
    "        # 2. Copy white_fly images\n",
    "        wf_split = whitefly_split_map[split]\n",
    "        wf_src = WHITEFLY_PATH / wf_split / 'white_fly'\n",
    "        if wf_src.exists():\n",
    "            count = 0\n",
    "            for img in wf_src.glob('*'):\n",
    "                if img.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                    shutil.copy2(img, unified_path / split / 'white_fly' / img.name)\n",
    "                    count += 1\n",
    "            stats[split]['white_fly'] = count\n",
    "            print(f\"  white_fly: {count} images\")\n",
    "        \n",
    "        # 3. Copy healthy FROM CATERPILLAR DATASET ONLY (no duplicates)\n",
    "        healthy_src = CATERPILLAR_PATH / cat_split / 'healthy'\n",
    "        if healthy_src.exists():\n",
    "            count = 0\n",
    "            for img in healthy_src.glob('*'):\n",
    "                if img.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                    shutil.copy2(img, unified_path / split / 'healthy' / img.name)\n",
    "                    count += 1\n",
    "            stats[split]['healthy'] = count\n",
    "            print(f\"  healthy: {count} images (from caterpillar dataset only)\")\n",
    "        \n",
    "        # 4. Copy not_coconut FROM CATERPILLAR DATASET ONLY (no duplicates)\n",
    "        nc_src = CATERPILLAR_PATH / cat_split / 'not_coconut'\n",
    "        if nc_src.exists():\n",
    "            count = 0\n",
    "            for img in nc_src.glob('*'):\n",
    "                if img.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                    shutil.copy2(img, unified_path / split / 'not_coconut' / img.name)\n",
    "                    count += 1\n",
    "            stats[split]['not_coconut'] = count\n",
    "            print(f\"  not_coconut: {count} images (from caterpillar dataset only)\")\n",
    "    \n",
    "    return unified_path, stats\n",
    "\n",
    "# Create the unified dataset\n",
    "print(\"=\"*60)\n",
    "print(\"Creating Unified Dataset v2 (No Duplicates)\")\n",
    "print(\"=\"*60)\n",
    "UNIFIED_DATA_PATH, dataset_stats = create_unified_dataset_v2()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset Creation Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "df_stats = pd.DataFrame(dataset_stats).T\n",
    "df_stats['Total'] = df_stats.sum(axis=1)\n",
    "df_stats.loc['Total'] = df_stats.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset Distribution (No Duplicates)\")\n",
    "print(\"=\"*60)\n",
    "print(df_stats.to_string())\n",
    "\n",
    "total_images = int(df_stats.loc['Total', 'Total'])\n",
    "train_count = int(df_stats.loc['train', 'Total'])\n",
    "val_count = int(df_stats.loc['validation', 'Total'])\n",
    "test_count = int(df_stats.loc['test', 'Total'])\n",
    "\n",
    "print(f\"\\nTotal Images: {total_images}\")\n",
    "print(f\"  Train: {train_count} ({train_count/total_images*100:.1f}%)\")\n",
    "print(f\"  Validation: {val_count} ({val_count/total_images*100:.1f}%)\")\n",
    "print(f\"  Test: {test_count} ({test_count/total_images*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Dataset Distribution (v2 - No Duplicates)', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db', '#f39c12']\n",
    "splits_to_plot = ['train', 'validation', 'test']\n",
    "\n",
    "for idx, split in enumerate(splits_to_plot):\n",
    "    ax = axes[idx]\n",
    "    split_data = df_stats.loc[split, CONFIG['classes']]\n",
    "    bars = ax.bar(CONFIG['classes'], split_data, color=colors)\n",
    "    ax.set_title(f'{split.capitalize()} Set', fontweight='bold')\n",
    "    ax.set_ylabel('Number of Images')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, split_data):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                f'{int(val)}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'dataset_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'dataset_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance pie chart\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "train_data = df_stats.loc['train', CONFIG['classes']]\n",
    "explode = (0.02, 0.02, 0.02, 0.02)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    train_data, \n",
    "    labels=CONFIG['classes'],\n",
    "    colors=colors,\n",
    "    explode=explode,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    pctdistance=0.75\n",
    ")\n",
    "ax.set_title('Training Set Class Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add legend with counts\n",
    "legend_labels = [f'{cls}: {int(train_data[cls])} images' for cls in CONFIG['classes']]\n",
    "ax.legend(wedges, legend_labels, loc='lower right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'class_distribution_pie.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'class_distribution_pie.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "fig.suptitle('Sample Images from Each Class', fontsize=14, fontweight='bold')\n",
    "\n",
    "for row, cls in enumerate(CONFIG['classes']):\n",
    "    class_path = UNIFIED_DATA_PATH / 'train' / cls\n",
    "    images = list(class_path.glob('*'))[:5]\n",
    "    \n",
    "    for col, img_path in enumerate(images):\n",
    "        ax = axes[row, col]\n",
    "        img = plt.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(cls, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'sample_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'sample_images.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Generators with Strong Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong data augmentation for training (prevent overfitting)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "# No augmentation for validation/test\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    UNIFIED_DATA_PATH / 'train',\n",
    "    target_size=CONFIG['img_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    classes=CONFIG['classes'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    UNIFIED_DATA_PATH / 'validation',\n",
    "    target_size=CONFIG['img_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    classes=CONFIG['classes'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    UNIFIED_DATA_PATH / 'test',\n",
    "    target_size=CONFIG['img_size'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    classes=CONFIG['classes'],\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Test samples: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show augmentation examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Data Augmentation Examples', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Get a sample image\n",
    "sample_path = list((UNIFIED_DATA_PATH / 'train' / 'caterpillar').glob('*'))[0]\n",
    "sample_img = plt.imread(sample_path)\n",
    "sample_img = np.expand_dims(sample_img, 0)\n",
    "\n",
    "axes[0, 0].imshow(sample_img[0].astype('uint8'))\n",
    "axes[0, 0].set_title('Original', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Generate augmented versions\n",
    "aug_gen = train_datagen.flow(sample_img, batch_size=1)\n",
    "for i in range(1, 10):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    aug_img = next(aug_gen)[0]\n",
    "    axes[row, col].imshow(aug_img)\n",
    "    axes[row, col].set_title(f'Augmented {i}')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'augmentation_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'augmentation_examples.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "y_train = train_generator.classes\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "print(\"\\nClass Weights (for balanced training):\")\n",
    "print(\"-\" * 40)\n",
    "for idx, cls in enumerate(CONFIG['classes']):\n",
    "    count = np.sum(y_train == idx)\n",
    "    weight = class_weights[idx]\n",
    "    print(f\"  {cls}: {count} samples -> weight: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Build EfficientNetB0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_model(num_classes=4, dropout_rate=0.4):\n",
    "    \"\"\"Build EfficientNetB0 model for transfer learning.\"\"\"\n",
    "    \n",
    "    # Load pre-trained EfficientNetB0\n",
    "    base_model = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = layers.GlobalAveragePooling2D(name='global_pool')(x)\n",
    "    \n",
    "    # Dense layers with dropout\n",
    "    x = layers.Dense(512, activation='relu', name='dense_1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='dropout_1')(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu', name='dense_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.Dropout(dropout_rate, name='dropout_2')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='EfficientNetB0_Unified_v2')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build the model\n",
    "model, base_model = build_efficientnet_model(\n",
    "    num_classes=len(CONFIG['classes']),\n",
    "    dropout_rate=CONFIG['dropout_rate']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Phase 1: Training with Frozen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: Training with Frozen Base Layers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate_phase1']),\n",
    "    loss=FocalLossWithSmoothing(\n",
    "        gamma=CONFIG['focal_gamma'],\n",
    "        label_smoothing=CONFIG['label_smoothing'],\n",
    "        num_classes=len(CONFIG['classes'])\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_phase1 = [\n",
    "    ModelCheckpoint(\n",
    "        str(MODEL_SAVE_PATH / 'best_model_phase1.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nTraining for {CONFIG['phase1_epochs']} epochs...\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate_phase1']}\")\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['phase1_epochs'],\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_phase1,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 1 Best Validation Accuracy: {max(history_phase1.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Phase 2: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PHASE 2: Fine-tuning (Unfreezing Top Layers)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze top layers of base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers, unfreeze later layers\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_count = sum([1 for layer in model.layers if layer.trainable])\n",
    "print(f\"Trainable layers: {trainable_count}\")\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['learning_rate_phase2']),\n",
    "    loss=FocalLossWithSmoothing(\n",
    "        gamma=CONFIG['focal_gamma'],\n",
    "        label_smoothing=CONFIG['label_smoothing'],\n",
    "        num_classes=len(CONFIG['classes'])\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks for fine-tuning\n",
    "callbacks_phase2 = [\n",
    "    ModelCheckpoint(\n",
    "        str(MODEL_SAVE_PATH / 'best_model.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\\nFine-tuning for {CONFIG['phase2_epochs']} epochs...\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate_phase2']}\")\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['phase2_epochs'],\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks_phase2,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nPhase 2 Best Validation Accuracy: {max(history_phase2.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "def combine_histories(h1, h2):\n",
    "    combined = {}\n",
    "    for key in h1.history.keys():\n",
    "        combined[key] = h1.history[key] + h2.history[key]\n",
    "    return combined\n",
    "\n",
    "full_history = combine_histories(history_phase1, history_phase2)\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Training History - Unified Model v2 (EfficientNetB0)', fontsize=14, fontweight='bold')\n",
    "\n",
    "epochs_range = range(1, len(full_history['accuracy']) + 1)\n",
    "phase1_end = len(history_phase1.history['accuracy'])\n",
    "\n",
    "# Accuracy\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(epochs_range, full_history['accuracy'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(epochs_range, full_history['val_accuracy'], 'r-', label='Val Accuracy', linewidth=2)\n",
    "ax1.axvline(x=phase1_end, color='g', linestyle='--', label='Phase 2 Start')\n",
    "ax1.set_title('Model Accuracy', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs_range, full_history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax2.plot(epochs_range, full_history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax2.axvline(x=phase1_end, color='g', linestyle='--', label='Phase 2 Start')\n",
    "ax2.set_title('Model Loss', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax3 = axes[1, 0]\n",
    "if 'lr' in full_history:\n",
    "    ax3.plot(epochs_range, full_history['lr'], 'g-', linewidth=2)\n",
    "elif 'learning_rate' in full_history:\n",
    "    ax3.plot(epochs_range, full_history['learning_rate'], 'g-', linewidth=2)\n",
    "ax3.axvline(x=phase1_end, color='r', linestyle='--', label='Phase 2 Start')\n",
    "ax3.set_title('Learning Rate Schedule', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Learning Rate')\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Train vs Val gap (overfitting check)\n",
    "ax4 = axes[1, 1]\n",
    "gap = [t - v for t, v in zip(full_history['accuracy'], full_history['val_accuracy'])]\n",
    "ax4.plot(epochs_range, gap, 'purple', linewidth=2)\n",
    "ax4.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(x=phase1_end, color='g', linestyle='--', label='Phase 2 Start')\n",
    "ax4.fill_between(epochs_range, gap, 0, alpha=0.3, color='purple')\n",
    "ax4.set_title('Overfitting Check (Train - Val Accuracy)', fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy Gap')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Model Evaluation on Test Set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "best_model = keras.models.load_model(\n",
    "    MODEL_SAVE_PATH / 'best_model.keras',\n",
    "    custom_objects={'FocalLossWithSmoothing': FocalLossWithSmoothing}\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_accuracy = best_model.evaluate(test_generator, verbose=1)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Predictions and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "test_generator.reset()\n",
    "y_pred_probs = best_model.predict(test_generator, verbose=1)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=CONFIG['classes'],\n",
    "    digits=4,\n",
    "    output_dict=True\n",
    ")\n",
    "print(classification_report(y_true, y_pred, target_names=CONFIG['classes'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Per-Class Metrics Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_data = []\n",
    "for cls in CONFIG['classes']:\n",
    "    metrics_data.append({\n",
    "        'Class': cls,\n",
    "        'Precision': report[cls]['precision'],\n",
    "        'Recall': report[cls]['recall'],\n",
    "        'F1-Score': report[cls]['f1-score'],\n",
    "        'Support': int(report[cls]['support'])\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Overall metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Metrics\")\n",
    "print(\"=\"*60)\n",
    "macro_precision = report['macro avg']['precision']\n",
    "macro_recall = report['macro avg']['recall']\n",
    "macro_f1 = report['macro avg']['f1-score']\n",
    "\n",
    "print(f\"  Accuracy:        {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"  Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Macro Recall:    {macro_recall:.4f}\")\n",
    "print(f\"  Macro F1-Score:  {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check (Madam's requirements)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Quality Check (Madam's Requirements)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if P, R, F1 are close for each class\n",
    "max_diff = 0\n",
    "for cls in CONFIG['classes']:\n",
    "    p = report[cls]['precision']\n",
    "    r = report[cls]['recall']\n",
    "    f1 = report[cls]['f1-score']\n",
    "    diff = max(abs(p-r), abs(r-f1), abs(p-f1))\n",
    "    max_diff = max(max_diff, diff)\n",
    "    status = \"OK\" if diff < 0.15 else \"WARNING\"\n",
    "    print(f\"  {cls}: P={p:.3f}, R={r:.3f}, F1={f1:.3f} -> Max diff: {diff:.3f} [{status}]\")\n",
    "\n",
    "# Check accuracy vs F1\n",
    "acc_f1_diff = abs(test_accuracy - macro_f1)\n",
    "print(f\"\\n  Accuracy vs Macro F1 difference: {acc_f1_diff:.4f}\")\n",
    "print(f\"  Accuracy close to F1: {acc_f1_diff < 0.05}\")\n",
    "\n",
    "if max_diff < 0.15:\n",
    "    print(\"\\n  [PASS] Metrics are well balanced!\")\n",
    "else:\n",
    "    print(f\"\\n  [WARNING] Some class imbalance detected (max diff: {max_diff:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CONFIG['classes'],\n",
    "            yticklabels=CONFIG['classes'],\n",
    "            ax=ax1)\n",
    "ax1.set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Normalized (percentages)\n",
    "ax2 = axes[1]\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=CONFIG['classes'],\n",
    "            yticklabels=CONFIG['classes'],\n",
    "            ax=ax2)\n",
    "ax2.set_title('Confusion Matrix (Percentages)', fontweight='bold')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Per-Class Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(CONFIG['classes']))\n",
    "width = 0.25\n",
    "\n",
    "precisions = [report[cls]['precision'] for cls in CONFIG['classes']]\n",
    "recalls = [report[cls]['recall'] for cls in CONFIG['classes']]\n",
    "f1_scores = [report[cls]['f1-score'] for cls in CONFIG['classes']]\n",
    "\n",
    "bars1 = ax.bar(x - width, precisions, width, label='Precision', color='#3498db')\n",
    "bars2 = ax.bar(x, recalls, width, label='Recall', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, f1_scores, width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Class', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Per-Class Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CONFIG['classes'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(y=0.9, color='gray', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "add_labels(bars3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'per_class_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(14, 14))\n",
    "fig.suptitle('Sample Predictions', fontsize=14, fontweight='bold')\n",
    "\n",
    "test_generator.reset()\n",
    "batch_x, batch_y = next(test_generator)\n",
    "predictions = best_model.predict(batch_x, verbose=0)\n",
    "\n",
    "for i in range(16):\n",
    "    row, col = i // 4, i % 4\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    img = batch_x[i]\n",
    "    true_label = CONFIG['classes'][np.argmax(batch_y[i])]\n",
    "    pred_label = CONFIG['classes'][np.argmax(predictions[i])]\n",
    "    confidence = np.max(predictions[i]) * 100\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    \n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)',\n",
    "                 color=color, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_SAVE_PATH / 'sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_SAVE_PATH / 'sample_predictions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Save Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model info\n",
    "model_info = {\n",
    "    'model_name': CONFIG['model_name'],\n",
    "    'version': 'v2',\n",
    "    'architecture': 'EfficientNetB0',\n",
    "    'classes': CONFIG['classes'],\n",
    "    'num_classes': len(CONFIG['classes']),\n",
    "    'input_size': list(CONFIG['img_size']) + [3],\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'macro_precision': float(macro_precision),\n",
    "    'macro_recall': float(macro_recall),\n",
    "    'macro_f1': float(macro_f1),\n",
    "    'per_class_metrics': {\n",
    "        cls: {\n",
    "            'precision': float(report[cls]['precision']),\n",
    "            'recall': float(report[cls]['recall']),\n",
    "            'f1-score': float(report[cls]['f1-score']),\n",
    "            'support': int(report[cls]['support'])\n",
    "        } for cls in CONFIG['classes']\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'phase1_epochs': CONFIG['phase1_epochs'],\n",
    "        'phase2_epochs': CONFIG['phase2_epochs'],\n",
    "        'focal_gamma': CONFIG['focal_gamma'],\n",
    "        'label_smoothing': CONFIG['label_smoothing'],\n",
    "        'dropout_rate': CONFIG['dropout_rate']\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'total_images': total_images,\n",
    "        'train_images': train_count,\n",
    "        'validation_images': val_count,\n",
    "        'test_images': test_count,\n",
    "        'note': 'No duplicate healthy/not_coconut data'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(MODEL_SAVE_PATH / 'model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"Model info saved to: {MODEL_SAVE_PATH / 'model_info.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"                    TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Model: {CONFIG['model_name']}\n",
    "Architecture: EfficientNetB0 (Transfer Learning)\n",
    "Classes: {', '.join(CONFIG['classes'])}\n",
    "\n",
    "Test Results:\n",
    "  Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\n",
    "  Precision: {macro_precision:.4f}\n",
    "  Recall:    {macro_recall:.4f}\n",
    "  F1-Score:  {macro_f1:.4f}\n",
    "\n",
    "Per-Class Performance:\n",
    "  caterpillar - P: {report['caterpillar']['precision']:.4f}, R: {report['caterpillar']['recall']:.4f}, F1: {report['caterpillar']['f1-score']:.4f}\n",
    "  healthy     - P: {report['healthy']['precision']:.4f}, R: {report['healthy']['recall']:.4f}, F1: {report['healthy']['f1-score']:.4f}\n",
    "  not_coconut - P: {report['not_coconut']['precision']:.4f}, R: {report['not_coconut']['recall']:.4f}, F1: {report['not_coconut']['f1-score']:.4f}\n",
    "  white_fly   - P: {report['white_fly']['precision']:.4f}, R: {report['white_fly']['recall']:.4f}, F1: {report['white_fly']['f1-score']:.4f}\n",
    "\n",
    "Saved Files:\n",
    "  - best_model.keras\n",
    "  - model_info.json\n",
    "  - training_history.png\n",
    "  - confusion_matrix.png\n",
    "  - per_class_metrics.png\n",
    "  - sample_predictions.png\n",
    "  - dataset_distribution.png\n",
    "  - class_distribution_pie.png\n",
    "  - sample_images.png\n",
    "  - augmentation_examples.png\n",
    "\n",
    "Model saved at: {MODEL_SAVE_PATH}\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
